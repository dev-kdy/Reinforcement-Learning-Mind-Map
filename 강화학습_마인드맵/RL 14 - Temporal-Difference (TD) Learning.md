상위: [[RL 00 - Reinforcement Learning Index]]  
관련: [[RL 10 - Value Function (V, Q)]], [[RL 13 - Monte Carlo Learning]], [[RL 15 - Q-learning]], [[RL 16 - SARSA]]

---

## What (정의)

Temporal-Difference(TD) Learning은  
에피소드가 끝나기 전에,  
다음 상태의 추정값을 이용해 현재 값을 조금씩 보정하는 방식의 학습 방법이다.

즉, 실제로 끝까지 Return을 계산하지 않고

- 지금 상태의 가치 추정 V(s_t) 또는 Q(s_t, a_t)를 기준으로
    
- 바로 다음에 관측한 보상 r_{t+1}과
    
- 다음 상태의 추정 가치 V(s_{t+1}) 또는 Q(s_{t+1}, a_{t+1})를 이용해
    
- 현재 추정치를 조금씩 수정해 나가는 부트스트랩(bootstrap) 방식이다.
    

요약하면

- Monte Carlo는 “에피소드 끝까지 다 보고 실제 Return으로 학습”
    
- TD는 “한 스텝 앞의 추정값을 끌어와서 지금 값을 보정하며 학습”이라고 볼 수 있다.
    

---

## Why (배경/목적)

TD Learning이 중요한 이유는  
에피소드 단위로만 학습하는 Monte Carlo 방식의 한계를 보완해 주기 때문이다.

1. 매 스텝마다 학습 가능
    
    - 에피소드가 끝날 때까지 기다릴 필요 없이  
        한 스텝 진행할 때마다 바로 업데이트할 수 있다.
        
    - 이 덕분에 온라인 학습, 스트리밍 환경에 잘 맞는다.
        
2. 에피소드가 길거나 끝나지 않는 문제에 적용 가능
    
    - 게임처럼 짧은 에피소드가 있는 환경은 Monte Carlo도 편하지만,
        
    - 금융 시계열, 운영 시스템처럼 사실상 무한히 이어지는 문제에서는  
        에피소드 끝을 기다리는 것이 애매하다.
        
    - TD는 현재와 다음 상태만 있으면 바로 학습할 수 있어 이런 문제에 적합하다.
        
3. 샘플 효율과 수렴 속도
    
    - 관측되는 즉시 정보를 반영하므로  
        같은 양의 데이터로 더 빠르게 수렴하는 경우가 많다.
        
    - 동적 계획법(DP)의 부트스트랩 아이디어와  
        샘플 기반 학습을 결합한 형태라 실용성이 높다.
        
4. 많은 핵심 알고리즘의 기반
    
    - [[RL 15 - Q-learning]], [[RL 16 - SARSA]] 같은  
        대표적인 Value-based 알고리즘이 모두 TD 업데이트를 사용한다.
        
    - Deep RL에서도 DQN, 많은 Actor-Critic 변형이  
        사실상 TD 아이디어 위에 서 있다고 볼 수 있다.
        

---

## How (활용)

### 1. 기본 TD 업데이트 아이디어

TD Learning의 전형적인 업데이트 형태는 다음과 같은 구조를 갖는다.

- 새로운 값 ← 현재 값 + 학습률 α × (목표값 − 현재 값)
    

여기서

- 현재 값: V(s_t) 또는 Q(s_t, a_t)
    
- 목표값: “한 스텝 앞 보상 + 다음 상태의 추정 가치”
    
- 학습률 α: 0 < α ≤ 1 범위의 스칼라
    

상태 가치 V(s)의 1-step TD를 예로 들면 대략 이렇게 이해하면 된다.

- 목표값: r_{t+1} + γ V(s_{t+1})
    
- TD 오차 δ_t: r_{t+1} + γ V(s_{t+1}) − V(s_t)
    
- 업데이트: V(s_t)를 δ_t 방향으로 조금 이동시킨다.
    

즉, “지금 생각한 가치”와  
“한 스텝 앞을 보고 계산한 가치”의 차이(오차)를 이용해  
현재 상태의 가치를 조금씩 수정해 나가는 구조다.

---

### 2. TD와 Q-learning / SARSA의 연결

TD 업데이트를 행동 가치 Q(s, a)에 적용한 것이  
[[RL 15 - Q-learning]], [[RL 16 - SARSA]] 같은 알고리즘들이다.

대표적인 두 가지를 개념적으로만 정리하면 다음과 같다.

1. SARSA (on-policy TD)
    
    - 다음 상태에서 실제로 정책이 선택한 행동 a_{t+1}을 사용해  
        목표값을 만든다.
        
    - 목표값 예시: r_{t+1} + γ Q(s_{t+1}, a_{t+1})
        
    - Q(s_t, a_t)를 이 목표값 쪽으로 조금 이동시킨다.
        
    - 현재 정책이 실제로 걷는 경로를 따라  
        on-policy로 TD 학습을 수행하는 형태다.
        
2. Q-learning (off-policy TD)
    
    - 다음 상태 s_{t+1}에서 가능한 행동들 중  
        Q값이 최대인 행동을 기준으로 목표값을 만든다.
        
    - 목표값 예시: r_{t+1} + γ max_{a'} Q(s_{t+1}, a')
        
    - Q(s_t, a_t)를 이 목표값 쪽으로 조금 이동시킨다.
        
    - 행동은 탐색을 섞어서 선택하더라도,  
        업데이트는 항상 “가장 좋은 행동을 했다고 가정한”  
        off-policy 방식이다.
        

두 알고리즘 모두

- “다음 상태의 Q 값을 이용해 현재 Q를 보정한다”는  
    TD 부트스트랩 아이디어를 공통으로 사용한다.
    

---

### 3. Monte Carlo Learning과의 대비

[[RL 13 - Monte Carlo Learning]]과 비교해서  
TD Learning의 위치를 정리하면 다음과 같다.

- Monte Carlo
    
    - 에피소드 끝까지 진행 후, 실제 Return으로 업데이트
        
    - 장점: Return 정의와 일치, 개념이 직관적, 편향이 적다.
        
    - 단점: 에피소드가 길거나 없으면 사용이 번거롭고,  
        Return의 분산이 커서 학습이 불안정할 수 있다.
        
- TD Learning
    
    - 에피소드가 끝나기 전에,  
        한 스텝 앞의 추정값을 이용해 점진적으로 업데이트
        
    - 장점: 매 스텝 학습 가능,  
        긴 에피소드·무한 에피소드에도 적용 가능,  
        샘플을 효율적으로 사용
        
    - 단점: 추정값에 다시 의존하는 부트스트랩 특성 때문에  
        이론적으로 약간의 편향이 생길 수 있다.
        

실제로는

- Monte Carlo와 TD를 극단으로 두고
    
- n-step TD, TD(λ), λ-return 같은 기법들을 통해  
    둘 사이를 조절하는 방식이 많이 연구된다.
    

---

### 4. 설계 및 실무 관점 요약

- TD Learning 핵심 문장 한 줄 정리
    
    - “미래 전체 Return을 기다리지 않고,  
        한 스텝 앞의 보상과 추정 가치를 끌어와  
        지금 가치를 조금씩 수정하는 학습 방법”
        
- 사용할 때 고민할 포인트
    
    - 학습률 α를 너무 크게 두면  
        TD 오차에 과하게 반응해서 불안정해질 수 있다.
        
    - 할인율 γ, 탐색 전략(ε-greedy 등)과 함께  
        안정적으로 수렴하도록 튜닝해야 한다.
        
- 큰 그림에서의 위치
    
    - Value Function([[RL 10 - Value Function (V, Q)]])을  
        어떻게 학습할지에 대한  
        두 큰 축이 Monte Carlo와 TD이고,
        
    - 여기서 TD를 택했을 때  
        Q-learning, SARSA, Actor-Critic, DQN 등  
        다양한 알고리즘이 파생된다고 보면  
        전체 RL 알고리즘 지도를 이해하기가 훨씬 쉬워진다.