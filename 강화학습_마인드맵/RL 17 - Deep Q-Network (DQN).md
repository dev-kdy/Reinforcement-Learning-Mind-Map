상위: [[RL 00 - Reinforcement Learning Index]]  
관련: [[RL 08 - Model-free vs Model-based RL]], [[RL 10 - Value Function (V, Q)]], [[RL 12 - Exploration vs Exploitation]], [[RL 14 - Temporal-Difference (TD) Learning]], [[RL 15 - Q-learning]], [[RL 18 - Experience Replay]], [[RL 19 - Target Network]]

---

## What (정의)

Deep Q-Network(DQN)은  
Q-learning([[RL 15 - Q-learning]])에 딥러닝을 결합한 알고리즘으로,  
Q(s, a)를 신경망으로 근사하여  
고차원 상태(이미지, 대규모 피처 벡터 등)에서도 동작 가능하게 만든 방법이다.

기본 아이디어는 다음과 같다.

- 상태 s를 입력으로 받는 신경망을 하나 두고  
    각 행동 a에 대한 Q(s, a)를 출력하도록 만든다.
    
- 이 신경망이 곧 Q 테이블을 대신하는 함수 근사기 역할을 하며,  
    TD 업데이트([[RL 14 - Temporal-Difference (TD) Learning]])를 통해 파라미터를 학습한다.
    
- 이때 학습의 안정성을 위해  
    Experience Replay([[RL 18 - Experience Replay]])와  
    Target Network([[RL 19 - Target Network]])라는 두 가지 핵심 트릭을 함께 사용한다.
    

요약하면, DQN은  
tabular Q-learning을 고차원, 복잡한 상태공간으로 확장하기 위한  
딥러닝 기반 value-based RL 알고리즘이다.

---

## Why (배경/목적)

Q-learning 자체는 간단하지만,  
Q 테이블로 Q(s, a)를 직접 저장하는 방식은  
상태 공간이 커지면 곧바로 한계에 부딪힌다.

- 상태가 이미지(예: 84×84 픽셀, 여러 프레임 스택)라면  
    가능한 s의 조합이 사실상 무한에 가깝다.
    
- 이런 환경에서 상태마다 Q(s, a)를 따로 저장하는 것은 불가능하고,  
    비슷한 상태들 사이에서 일반화도 필요하다.
    

DQN이 필요한 이유는 다음과 같다.

1. 고차원 상태에서의 일반화
    
    - CNN, MLP 같은 딥네트워크는  
        비슷한 상태들에서 비슷한 Q값을 내도록 학습할 수 있다.
        
    - 이는 “한 번 본 상태” 근처의 상태들에도  
        그 학습 효과를 전이시켜 주므로  
        상태공간이 커도 어느 정도 커버가 가능해진다.
        
2. 이미지 기반 문제 해결
    
    - Atari 게임, 비디오 입력, 복잡한 관측값 등  
        고차원 입력을 직접 받아 처리할 수 있어  
        고전적인 Q-learning의 영역을 훨씬 넓혀 준다.
        
3. end-to-end 학습
    
    - 원시 픽셀 → 상태 표현 → Q값  
        모든 과정을 하나의 네트워크로 end-to-end 학습할 수 있다.
        
    - 별도의 수작업 피처 엔지니어링 없이도  
        행동 가치 함수를 직접 근사할 수 있다는 장점이 있다.
        

다만, 신경망과 TD 부트스트랩을 그대로 결합하면  
학습이 매우 불안정해지기 쉽기 때문에,  
이를 완화하기 위해 [[RL 18 - Experience Replay]], [[RL 19 - Target Network]] 같은  
안정화 기법이 함께 도입된다.

---

## How (활용)

### 1. 네트워크 구조의 역할

DQN에서 사용하는 Q 네트워크는  
상태 s를 받아 각 행동에 대한 Q값 벡터를 출력한다.

- 이산형 행동 공간 가정
    
    - 예: 행동이 N개라면  
        네트워크 출력 차원도 N으로 설정하고  
        각 차원은 Q(s, a_0), Q(s, a_1), ..., Q(s, a_{N−1})에 해당한다.
        
- 상태 인코딩 방식
    
    - 이미지 입력
        
        - CNN으로 공간적 구조를 인코딩한 뒤  
            Fully Connected 층을 거쳐 Q값 출력
            
    - 벡터 입력
        
        - MLP로 몇 개의 은닉층을 통과시켜 Q값 출력
            

행동 선택은 보통 다음과 같이 진행된다.

- 현재 상태 s를 네트워크에 넣어 Q(s, ·)를 얻고
    
- [[RL 12 - Exploration vs Exploitation]]에서 설명한 ε-greedy 정책으로  
    행동을 선택한다.
    
    - 확률 1−ε: argmax_a Q(s, a)
        
    - 확률 ε: 랜덤 행동
        

즉, DQN은 구조적으로는 Q-learning과 동일하지만  
Q를 테이블 대신 네트워크로 표현한다.

---

### 2. TD 학습, Experience Replay, Target Network

DQN 학습 루프의 핵심 구성은 다음 세 가지로 정리할 수 있다.

1. 경험 수집과 Experience Replay([[RL 18 - Experience Replay]])
    
    - 에이전트가 환경과 상호작용하면서  
        (s, a, r, s', done) 형태의 transition을 계속 모은다.
        
    - 이 transition들을 버퍼(Replay Memory)에 쌓아 두고,  
        학습 시에는 이 버퍼에서 미니배치를 랜덤 샘플링해 사용한다.
        
    - 효과
        
        - 데이터 간 상관관계를 줄여 학습 안정성 향상
            
        - 과거 경험을 여러 번 재사용해 샘플 효율 개선
            
2. Target Network([[RL 19 - Target Network]]) 사용
    
    - Q(s, a)를 예측하는 온라인 네트워크와  
        목표값 계산에만 사용하는 타깃 네트워크를 분리한다.
        
    - 타깃 네트워크는 일정 주기마다  
        온라인 네트워크의 파라미터로 업데이트한다.
        
    - 효과
        
        - 목표값에 사용하는 Q가 자꾸 변하지 않도록 해  
            부트스트랩으로 인한 불안정을 줄인다.
            
3. TD 손실로 네트워크 업데이트
    
    - 샘플 (s, a, r, s', done)을 이용해  
        TD 목표값을 만든다.
        
    - done이 아닌 경우
        
        - 목표값 ≈ r + γ max_{a'} Q_target(s', a')
            
    - done인 경우
        
        - 목표값 ≈ r
            
    - 온라인 네트워크의 Q_pred(s, a)와  
        이 목표값의 차이를 줄이도록  
        MSE 형태의 손실을 최소화하는 방향으로  
        파라미터를 업데이트한다.
        

이 흐름을 계속 반복하면서

- Q 네트워크가 점점 더 정확한 Q(s, a)를 근사하고
    
- ε-greedy 정책이 점차 좋은 행동을 선택하게 된다.
    

---

### 3. 설계 및 실무 관점 포인트

DQN을 이해하거나 구현할 때 함께 고려하면 좋은 포인트들이다.

1. 상태 전처리
    
    - 이미지인 경우  
        흑백 변환, 리사이즈, 프레임 스택(최근 몇 프레임 묶기) 등  
        기본적인 전처리가 중요하다.
        
    - 입력 스케일, 정규화 수준도 학습 안정성에 영향을 준다.
        
2. ε 스케줄 관리
    
    - 초기에는 ε를 크게 두어 넓게 탐색하고
        
    - 학습이 진행되면서 ε를 서서히 줄여  
        Exploitation 비중을 높이는 패턴을 많이 사용한다.
        
3. DQN 변형들
    
    - Double DQN
        
        - max 연산으로 인한 과추정 문제를 완화하기 위한 변형
            
    - Dueling DQN
        
        - 상태 가치와 Advantage를 분리해 표현하여  
            효율적인 가치 추정을 노리는 구조
            
    - Prioritized Experience Replay
        
        - TD 오차가 큰 경험을 더 자주 샘플링해  
            학습 효율을 높이는 방법
            

이런 변형들은 모두  
기본 DQN 구조 위에 얹을 수 있는 확장 요소로,  
실전에서는 DQN을 그대로 쓰기보다  
이런 변형들을 섞어 사용하는 경우가 많다.

4. 한계와 주의점
    
    - 이산 행동 공간을 전제로 설계되어 있어  
        연속 행동 환경에는 바로 적용하기 어렵다.  
        (이 경우 DDPG, SAC, PPO 등으로 넘어가는 것이 자연스럽다.)
        
    - Q값을 부트스트랩과 신경망으로 함께 학습하기 때문에  
        하이퍼파라미터(학습률, γ, 배치 크기, 타깃 업데이트 주기 등)를  
        잘못 설정하면 불안정해지기 쉽다.
        

---

정리하면, DQN은

- Q-learning([[RL 15 - Q-learning]])의 구조를 유지하면서
    
- Q(s, a)를 신경망으로 근사하고
    
- [[RL 18 - Experience Replay]], [[RL 19 - Target Network]]로 안정성을 확보한
    

대표적인 deep value-based RL 알고리즘이며,  
이미지 기반 게임 환경 등에서  
강화학습과 딥러닝의 결합을 상징하는 대표 사례로 자리 잡은 방법이다.