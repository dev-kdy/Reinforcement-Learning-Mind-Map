상위: [[RL 00 - Reinforcement Learning Index]]  
관련: [[RL 01 - Reinforcement Learning]], [[RL 06 - Reward]], [[RL 07 - Return & Discount Factor γ]], [[RL 10 - Value Function (V, Q)]], [[RL 11 - Advantage Function]], [[RL 14 - Temporal-Difference (TD) Learning]]

---

## What (정의)

Monte Carlo Learning은  
에피소드가 끝날 때까지 기다렸다가,  
실제 Return을 계산해 [[RL 10 - Value Function (V, Q)]]를 업데이트하는 방식이다.

조금 더 풀어 쓰면 다음과 같다.

- 하나의 에피소드에서  
    시점 t 이후로 받는 보상의 (할인된) 누적합을  
    `G_t`라고 할 때,  
    Monte Carlo는 이 `G_t`를 직접 계산해서  
    V(s) 또는 Q(s, a)를 추정한다.
    
- 상태 가치의 경우  
    특정 정책 π에 대해  
    상태 s에서 시작했을 때의 기대 Return  
    `V^π(s) = E[G_t | S_t = s]`를  
    여러 에피소드의 `G_t` 평균으로 근사한다.
    
- 행동 가치의 경우  
    `Q^π(s, a) = E[G_t | S_t = s, A_t = a]`를  
    같은 방식으로 근사한다.
    

요약하면,  
에피소드별로 실제로 얻은 Return을 그대로 써서  
Value Function을 “표본 평균” 방식으로 추정하는 학습 방법이다.

---

## Why (배경/목적)

Monte Carlo 방식은 다음과 같은 이유로 중요하다.

1. 개념적으로 직관적
    
    - “에피소드가 끝날 때까지 쭉 해 보고,  
        실제로 얼마 벌었는지 평균을 내자”라는 아이디어라  
        Return과 Value의 정의와 바로 연결된다.
        
    - 수학적으로도 기대값의 표본 평균이라는  
        아주 익숙한 개념이라 이해가 쉽다.
        
2. 완전한 Return을 활용
    
    - TD처럼 추정치에 다시 의존하지 않고  
        실제로 관측된 Return을 그대로 사용한다.
        
    - 이 때문에 편향은 적고,  
        “정책이 실제로 어떤 성과를 내는지”에 충실한 업데이트를 한다.
        
3. 에피소드가 명확히 끝나는 환경에 잘 맞음
    
    - 에피소드의 시작과 끝이 뚜렷한 게임,  
        시뮬레이션 환경에서는  
        구현도 단순하고 디버깅도 쉽다.
        

반면, 한 스텝마다 바로바로 업데이트하는 [[RL 14 - Temporal-Difference (TD) Learning]]과 대비되며,  
두 방식의 장단점을 비교하는 것이  
강화학습 학습 패러다임을 이해하는 핵심 포인트 중 하나다.

---

## How (활용)

### 1. 기본 아이디어: 에피소드 단위 평균

Monte Carlo Learning의 기본 흐름은 다음과 같다.

1. 정책 π로 에피소드를 하나 실행한다.
    
    - 상태, 행동, 보상 시퀀스를 얻는다.  
        예: `(s_0, a_0, r_1, s_1, a_1, r_2, ..., s_T)`
        
2. 각 시점 t에 대해 Return `G_t`를 계산한다.
    
    - 에피소드의 끝 T까지의 보상을  
        할인 계수 γ를 사용해 누적합으로 만든다.
        
    - 예:
        
        - `G_t = r_{t+1} + γ r_{t+2} + γ² r_{t+3} + ... + γ^{T-t-1} r_T`
            
3. 같은 상태(또는 상태–행동 쌍)에 대해  
    여러 에피소드에서 얻은 `G_t` 값들을 모아서  
    평균을 내고 V(s), Q(s, a)를 갱신한다.
    
    - 직관적으로 “이 상태(또는 행동)를 여러 번 겪어 보니  
        평균적으로 이 정도 Return이 나오더라”라고 학습하는 것.
        
4. 충분히 많은 에피소드를 반복하면  
    표본 평균이 기대값에 수렴한다는 원리로  
    V^π, Q^π에 가까운 값을 얻게 된다.
    

이때, 동일 상태를 어떻게 처리하느냐에 따라  
두 가지 변형이 자주 언급된다.

- First-visit Monte Carlo
    
    - 하나의 에피소드에서  
        어떤 상태 s가 처음 등장했을 때의 `G_t`만 사용해 평균을 낸다.
        
- Every-visit Monte Carlo
    
    - 에피소드 내에서 상태 s를 방문할 때마다  
        해당 시점의 `G_t`를 모두 사용해 평균을 낸다.
        

둘 다 결국 “여러 번 방문한 상태의 Return 평균”이라는 관점은 같다.

---

### 2. TD Learning과의 대비

[[RL 14 - Temporal-Difference (TD) Learning]]과 비교했을 때  
Monte Carlo 방식의 특징을 정리하면 다음과 같다.

- Monte Carlo
    
    - 에피소드가 끝날 때까지 기다렸다가 업데이트
        
    - 실제 Return을 사용 → 이론적으로는 편향이 적음
        
    - 에피소드가 길거나, 잘 끝나지 않는 환경에서는 불편
        
    - 샘플 분산이 크고, 수렴 속도가 느릴 수 있다.
        
- TD Learning
    
    - 한 스텝(또는 몇 스텝)마다 바로 업데이트
        
    - 현재 추정한 V(s') 또는 Q(s', a')에 의존  
        → 약간의 편향이 있지만,  
        샘플 효율과 수렴 속도가 좋은 편
        
    - 에피소드가 무한히 길거나  
        명확한 종료가 없어도 적용 가능
        

따라서

- “완전히 에피소드 단위로 생각하기 쉬운 문제”에서는  
    Monte Carlo 방식이 교육용, 베이스라인으로 좋고
    
- “온라인, 스트리밍, 에피소드가 명확하지 않은 문제”에서는  
    TD 방식이 더 실용적이다.
    

---

### 3. 실무/설계 관점에서의 활용 포인트

- Monte Carlo를 먼저 공부하는 이유
    
    - Return, Value, Policy의 관계를  
        가장 직관적으로 체험할 수 있게 해 준다.
        
    - 이를 이해해 두면  
        이후 TD, Q-learning, Actor-Critic을 볼 때  
        “결국 Return 근사를 어떤 방식으로 하느냐” 관점에서  
        공통 구조가 잘 보인다.
        
- 환경 특성에 맞춰 선택
    
    - 에피소드가 짧고, 시뮬레이션이 빠른 환경  
        → Monte Carlo 기반 방법도 충분히 유용
        
    - 에피소드가 매우 길거나,  
        중간에 끊어서도 의미 있는 업데이트를 하고 싶다면  
        → TD, n-step, λ-return 같은 변형으로 넘어가는 것이 자연스럽다.
        
- Value Function과의 연결
    
    - Monte Carlo는 [[RL 10 - Value Function (V, Q)]]의 정의를  
        “그대로 구현한 것”이라는 관점에서  
        Value 개념을 몸으로 느끼기에 좋은 출발점이다.