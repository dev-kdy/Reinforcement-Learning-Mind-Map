상위: [[RL 00 - Reinforcement Learning Index]]  
관련: [[RL 01 - Reinforcement Learning]], [[RL 03 - Environment & MDP]], [[RL 04 - State]], [[RL 05 - Action & Action Space]], [[RL 09 - Policy (정책)]], [[RL 10 - Value Function (V, Q)]]

---

## What (정의)

Agent는  
환경([[RL 03 - Environment & MDP]])[^1]이 제공하는 상태([[RL 04 - State]])[^2]를 관찰하고,  
행동 공간([[RL 05 - Action & Action Space]])[^3]에서 행동을 선택하는 의사결정 주체다.[^4]  

강화학습 안에서 Agent는 다음과 같이 이해할 수 있다.

- 매 시점 t마다 상태 `s_t`를 입력으로 받아 행동 `a_t`를 출력하는 함수처럼 동작한다.  
  이를 정책([[RL 09 - Policy (정책)]]) π(a|s)[^5]로 표현한다.

- Agent 내부에는  
  - 정책 π(a|s)를 표현하는 구조  
  - 상태/행동의 가치를 추정하는 가치 함수([[RL 10 - Value Function (V, Q)]]) V(s), Q(s, a)[^6]  
  - 필요하다면 경험을 저장하는 메모리(replay buffer 등)[^7]  
  가 포함될 수 있다.

- 같은 환경이라도 Agent 설계에 따라  
  완전히 다른 행동 전략과 성능을 보일 수 있다.

---

## Why (배경/목적)

Agent 개념을 분리해서 정의하면  
강화학습 문제를 “세계(환경)는 그대로 두고, 누가 어떻게 행동하느냐”의 관점에서  
깔끔하게 바라볼 수 있다.

- 같은 환경에서 서로 다른 Agent를 비교할 수 있다.  
  - 예: 동일한 게임 환경에 대해  
    규칙 기반 봇, Q-learning Agent, DQN Agent, PPO Agent의 성능 비교

- “무엇을 최적화할 것인가”를 Agent에 명시적으로 심을 수 있다.  
  - 단기 점수 최대화  
  - 장기적인 사용자 유지율, 수익, 안전성, 공정성 등 복합 목표

- 환경 설계와 학습 알고리즘 설계를 분리할 수 있다.  
  - Environment는 상태 전이와 보상 구조를 정의하는 세계  
  - Agent는 그 위에서 학습·탐색을 수행하는 의사결정 시스템

- 멀티 에이전트 환경에서도  
  각 플레이어를 하나의 Agent로 모델링하여  
  상호작용과 경쟁/협력을 분석하기 쉽다.

결국 Agent를 중심에 두면  
“어떤 플레이어를 설계할 것인가?”라는 관점에서  
강화학습 알고리즘과 구조를 비교·설계하기가 훨씬 수월해진다.

---

## How (활용)

### 1. 강화학습 루프에서의 역할

Agent는 기본 상호작용 루프에서 다음 세 가지 역할을 수행한다.

1. 관찰  
   - 현재 상태 `s_t`를 관찰한다.  
   - 경우에 따라 과거 히스토리나 내부 메모리까지 함께 이용해  
     “지금 상황”을 판단한다.

2. 행동 선택  
   - 현재 정책 π(a|s)에 따라 행동 `a_t`를 선택한다.  
   - 예:  
     - Q(s, a)가 최대가 되는 행동 선택  
     - 확률 분포 π(a|s)에서 샘플링  
     - ε-greedy처럼 탐색을 섞은 전략 사용[^8]

3. 학습(업데이트)  
   - 환경으로부터 다음 상태 `s_{t+1}`와 보상 `r_{t+1}`을 받은 뒤,  
     경험 `(s_t, a_t, r_{t+1}, s_{t+1})`을 이용해  
     정책과 가치 함수의 파라미터를 업데이트한다.  
   - 이때 사용하는 구체적인 학습 규칙이  
     [[RL 13 - Monte Carlo Learning]], [[RL 14 - Temporal-Difference (TD) Learning]] 기반 알고리즘들이다.

---

### 2. 적용 사례

1. 게임 플레이 에이전트  
   - 관찰: 게임 화면, 체력, 남은 시간 등 상태를 읽는다.  
   - 행동: 이동, 점프, 공격 등 버튼 입력을 결정한다.  
   - 학습: 이긴 게임에서는 해당 전략을 강화하고,  
     진 게임에서는 다른 행동을 탐색하도록 정책을 조정한다.

2. 로봇 제어 에이전트  
   - 관찰: 센서 값, 위치, 속도 등 물리 상태를 읽는다.  
   - 행동: 모터 토크, 회전 각도, 속도 명령 등을 결정한다.  
   - 학습: 목표에 더 빨리·안전하게 도달하는 행동일수록  
     더 큰 누적 보상을 받도록 정책을 수정한다.

3. 추천 시스템 에이전트  
   - 관찰: 사용자 프로필, 최근 클릭/시청 이력, 시간대 등 상태를 읽는다.  
   - 행동: 어떤 콘텐츠·상품·광고를 노출할지 선택한다.  
   - 학습: 클릭·구매·이탈 여부 등 피드백에 따라  
     “어떤 사용자에게 무엇을 언제 보여줄지” 정책을 개선한다.

4. 운영 최적화 에이전트  
   - 관찰: 서버 부하, 재고 수준, 주문량 등 상태를 읽는다.  
   - 행동: 서버 증설/축소, 재고 발주량 조절, 배치 스케줄 결정 등  
     운영 관련 의사결정을 내린다.  
   - 학습: 비용·지연·서비스 품질에 기반한 보상을 통해  
     장기적으로 유리한 운영 전략을 배우게 된다.

---

### 3. 구현 관점에서의 활용 포인트

- Agent 클래스 구조  
  - 코드에서는 보통 `Agent` 클래스를 두고  
    - `act(state)` 메서드가 행동을 반환  
    - `learn(experience)` 또는 `update(...)` 메서드가  
      정책/가치 함수 파라미터를 업데이트하도록 구현한다.

- 정책 표현 방식  
  - 테이블 기반(Q-table)  
  - 함수 근사 기반(신경망, 선형 모델 등)  
  - 결정적 정책 vs 확률적 정책

- 가치 함수와의 관계  
  - Value-based Agent: Q(s, a)에서 argmax로 행동 선택  
  - Policy-based Agent: π(a|s)를 직접 모델링  
  - Actor-Critic Agent: 정책과 가치 함수를 동시에 사용하여  
    학습 안정성과 성능을 모두 노린다.

- 탐색 전략 설계  
  - ε-greedy, softmax/Boltzmann, entropy 보너스 등  
  - “얼마나 새로운 행동을 시도할 것인가”를 Agent 내부 로직으로 포함한다.

- 경험 저장 및 재사용  
  - DQN 계열에서는 replay buffer를 두어  
    과거 경험을 샘플링하면서 학습 안정성과 샘플 효율을 높인다.

---

[^1]: 환경(Environment) — 에이전트의 행동에 반응하여 상태 전이와 보상을 돌려주는 세계.  
[^2]: 상태(State) — 현재 시점의 상황을 나타내는 정보 묶음.  
[^3]: 행동 공간(Action Space) — 에이전트가 선택할 수 있는 모든 행동의 집합.  
[^4]: 에이전트(Agent) — 상태를 관찰하고 행동을 선택하며 보상을 기준으로 학습하는 의사결정 주체.  
[^5]: 정책(Policy) — 상태를 입력받아 어떤 행동을 선택할지 결정하는 규칙 또는 확률 분포 π(a|s).  
[^6]: 가치 함수(Value Function) — 특정 상태 또는 상태–행동 쌍이 앞으로 가져올 누적 보상의 기대값 V(s), Q(s, a)를 나타내는 함수.  
[^7]: 경험 메모리(Replay Buffer) — 과거 경험을 저장해 두었다가 샘플링하며 학습에 사용하는 구조.  
[^8]: 탐색(Exploration) — 아직 충분히 시도하지 않은 행동을 선택해 정보를 얻는 과정. ε-greedy, softmax 선택 등이 대표적이다.