상위: [[RL 00 - Reinforcement Learning Index]]  
관련: [[RL 01 - Reinforcement Learning]], [[RL 02 - Agent]], [[RL 03 - Environment & MDP]], [[RL 05 - Action & Action Space]], [[RL 09 - Policy (정책)]], [[RL 10 - Value Function (V, Q)]]

---

## What (정의)

State는  
Agent([[RL 02 - Agent]])가 의사결정을 내릴 때 보게 되는 현재 상황의 요약 정보다.

시간 t에서의 상태를 `S_t` 또는 `s_t`라고 쓸 때, 이 상태에는  
앞으로의 보상, 상태 전이, 최적 행동 선택에 영향을 주는 중요한 정보들이  
가능한 한 빠짐없이 담겨 있어야 한다.

MDP([[RL 03 - Environment & MDP]]) 관점에서 보면 상태는 다음을 만족하도록 정의하는 것이 이상적이다.

- 현재 상태 `S_t`와 현재 행동 `A_t`를 알면  
    다음 상태 `S_{t+1}`와 보상 `R_{t+1}`의 분포가 결정된다.
    
- 즉, 과거의 세부 이력 전체 대신  
    현재 상태만으로도 미래를 예측할 수 있도록  
    정보를 잘 압축한 표현이다.
    

요약하면, State는  
지금 이 순간 의사결정을 내리기 위해  
환경에서 꺼내온 “정보 스냅샷”이라고 볼 수 있다.

---

## Why (배경/목적)

의사결정을 잘하려면  
필요한 정보가 충분히 포함된 상태 표현이 필요하다.

상태 설계가 부족하면  
정책([[RL 09 - Policy (정책)])이 아무리 복잡하고 강력해도  
좋은 성능을 내기 어렵다.

- 정보가 너무 부족한 상태
    
    - 예: 로봇의 위치만 알고 속도는 모르는 상태
        
    - 미래 움직임을 예측하기 어렵고, 최적 제어가 힘들어진다.
        
- 정보가 너무 많은 상태
    
    - 예: 불필요한 로그, 식별자, 잡음이 모두 포함된 거대한 벡터
        
    - 학습이 느려지고, 샘플 효율과 일반화 성능이 떨어진다.
        

좋은 상태 표현은 다음을 동시에 만족하려고 노력한다.

- 미래 보상과 전이에 중요한 정보는 최대한 포함
    
- 불필요하거나 중복된 정보는 최대한 제거
    
- 학습 알고리즘이 다루기 적당한 차원과 구조로 표현
    

특히 [[RL 10 - Value Function (V, Q)]], [[RL 17 - Deep Q-Network (DQN)]]처럼  
함수 근사 기반 방법에서는  
상태 표현이 곧 입력 feature가 되기 때문에  
성능에 미치는 영향이 매우 크다.

---

## How (활용)

### 1. 상태 표현의 형태

State는 문제와 환경에 따라 다양한 형태를 가질 수 있다.

- 이미지
    
    - 예: Atari 게임 화면, 자율주행 차량의 카메라 영상
        
    - 보통 그레이스케일 변환, 리사이즈, 정규화 등을 거쳐  
        신경망의 입력으로 사용한다.
        
- 수치 벡터
    
    - 예: 위치, 속도, 각도, 센서 값, 각종 지표들
        
    - 로봇 제어, 운영 최적화, 금융 트레이딩 등에서 많이 사용한다.
        
- 심볼릭/범주형 정보
    
    - 예: 현재 단계, 상태 플래그, 모드
        
    - 원-핫 인코딩이나 임베딩을 통해 벡터로 변환한다.
        
- 텍스트 기반 표현
    
    - 예: 사용자 최근 질의, 대화 이력의 요약
        
    - NLP 모델을 통해 임베딩 벡터로 변환해 상태로 사용할 수 있다.
        
- 혼합형
    
    - 위의 요소들을 합쳐 하나의 큰 벡터 또는 구조화된 입력으로 사용한다.
        

결국 구현 단계에서는  
대부분 하나의 실수 벡터 `x ∈ R^n` 형태로 인코딩되어  
정책, 가치 함수의 입력으로 사용된다.

---

### 2. 상태 설계 관점에서의 포인트

상태를 설계할 때는 다음 질문들을 체크리스트처럼 활용할 수 있다.

- 어떤 정보가 미래 보상과 직접적으로 연관되어 있는가
    
    - 예: 게임 점수, 체력, 남은 시간, 목표까지 거리 등
        
- 어떤 정보는 과감히 버려도 되는가
    
    - 예: 내부 ID, 사용자 이름처럼 의사결정에 영향 없는 것들
        
- 현재 관측만으로 충분한가, 아니면 히스토리가 필요할까
    
    - 현재 관측만으로는 부족하면  
        최근 몇 단계 관측을 함께 묶거나  
        RNN, LSTM으로 과거를 요약해 hidden state를 상태처럼 사용하기도 한다.
        
- 연속 값, 범주형 값, 텍스트 등을  
    어떻게 하나의 일관된 표현으로 합칠 것인가
    
    - 정규화, 스케일링, 임베딩 전략이 필요하다.
        

이 질문들을 통해  
“지금 정의한 State가 이 문제를 MDP에 최대한 가깝게 만들어 주는가?”를  
계속 점검하는 것이 중요하다.

---

### 3. 예시 도메인별 State

- 게임 환경
    
    - 요소: 캐릭터 위치·속도, 체력, 남은 시간, 적 위치, 맵 상태 등
        
    - 원시 픽셀 대신 이런 요약 정보를 쓰면  
        더 빠르고 안정적인 학습이 가능해지는 경우가 많다.
        
- 로봇 제어
    
    - 요소: 로봇의 관절 각도, 각속도, 위치, 속도, 센서 값 등
        
    - 장애물까지의 거리, 목표 지점 상대 위치 같은  
        고수준 feature를 추가로 포함할 수 있다.
        
- 추천 시스템
    
    - 요소: 사용자 최근 클릭/시청/구매 이력, 시간대, 디바이스, 위치,  
        간단한 프로필 정보 등
        
    - 이들을 하나의 임베딩 벡터로 압축해  
        상태로 사용하면 정책과 가치 함수가 이를 입력으로 사용한다.
        
- 운영/시스템 최적화
    
    - 요소: CPU/메모리 사용량, 큐 길이, 트래픽 양, 에러율,  
        재고 수준, 주문 속도 등
        
    - 장기적인 비용과 서비스 품질에 영향을 주는 지표 위주로 선택한다.
        

---

### 4. 구현 관점에서의 실무 팁

- 관측과 상태를 구분해서 생각하기
    
    - 관측(observation): 환경이 바로 내주는 원시 데이터
        
    - 상태(state): 이 데이터를 전처리·압축·변환해서  
        학습하기 좋은 형태로 만든 표현
        
- 전처리 파이프라인을 명시적으로 모듈화
    
    - 예: `preprocess(observation) -> state` 함수를 별도로 두고,  
        Agent는 항상 이 state만 보고 의사결정을 내리게 만든다.
        
- 차원과 복잡도를 점진적으로 늘리기
    
    - 처음부터 매우 복잡한 상태 표현을 쓰기보다  
        최소한의 정보로 시작해서  
        성능을 보며 점진적으로 feature를 추가하는 방식이  
        디버깅과 이해에 유리하다.
        
- [[RL 17 - Deep Q-Network (DQN)]]처럼 딥러닝 기반 알고리즘에서는
    
    - 정규화, 스케일링, 클리핑, 프레임 스태킹 등 전처리 전략이  
        학습 안정성과 수렴 속도에 큰 영향을 준다.