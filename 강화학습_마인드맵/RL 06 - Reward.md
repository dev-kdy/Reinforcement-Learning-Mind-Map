상위: [[RL 00 - Reinforcement Learning Index]]  
관련: [[RL 01 - Reinforcement Learning]], [[RL 07 - Return & Discount Factor γ]], [[RL 10 - Value Function (V, Q)]], [[RL 11 - Advantage Function]], [[RL 15 - Q-learning]], [[RL 17 - Deep Q-Network (DQN)]], [[RL 20 - Policy Gradient (기본 PG)]]

---

## What (정의)

Reward는  
방금 선택한 행동이 얼마나 좋았는지 또는 나빴는지에 대해  
환경이 에이전트에게 돌려주는 즉각적인 숫자 피드백이다.

시간 t에서 에이전트가 상태 `s_t`에서 행동 `a_t`를 했을 때  
환경은 다음 상태 `s_{t+1}`과 함께  
스칼라 값 `r_{t+1}`을 돌려준다.  
이 `r_{t+1}`이 바로 Reward다.

- 값이 클수록 방금 행동이 바람직했다는 의미
    
- 값이 작거나 음수일수록 바람직하지 않았다는 의미
    
- 보통 실수 하나로 표현되며, 에이전트의 학습 신호가 된다.
    

강화학습 전체의 목표는  
단순히 한 번의 Reward를 크게 만드는 것이 아니라,  
미래까지 포함한 Reward들의 합(또는 할인합)을  
최대한 크게 만드는 정책을 찾는 것이다.

---

## Why (배경/목적)

지도학습에서는 정답 라벨이  
입력마다 직접 주어지지만,  
강화학습에서는 매 시점마다 “정답 행동”이 딱 떨어지게 주어지지 않는다.

대신 Reward가 다음과 같은 역할을 한다.

- 방금 행동이 좋은 방향이었는지 나쁜 방향이었는지 알려주는 신호
    
- 여러 시점에 걸친 행동들의 결과를  
    하나의 숫자 흐름으로 요약한 피드백
    
- [[RL 10 - Value Function (V, Q)]], [[RL 11 - Advantage Function]]의  
    학습 목표가 되는 기준 값
    

에이전트는

- 좋은 행동 뒤에 나오는 Reward를 더 자주 받도록 정책을 바꾸고
    
- 나쁜 행동 뒤에 나오는 Reward를 덜 받도록 정책을 수정한다.
    

Reward 설계는 강화학습 문제 정의 그 자체라고 해도 될 정도로 중요하다.

- Reward가 목표를 잘 반영하면  
    학습된 정책도 자연스럽게 원하는 목표를 수행한다.
    
- Reward가 왜곡되어 있으면  
    에이전트가 “숫자를 최대화하지만, 사람이 원하는 행동과는 다른”  
    전략을 학습하는 이른바 Reward hacking이 발생할 수 있다.
    

---

## How (활용)

### 1. 환경 인터페이스에서의 Reward

OpenAI Gym 스타일 환경에서는  
`env.step(action)` 호출 결과로 Reward가 함께 반환된다.

- 반환 튜플 구조 예시(개념적으로)
    
    - 다음 상태: `next_state`
        
    - 보상: `reward`
        
    - 종료 플래그: `done`
        
    - 추가 정보: `info`
        

에이전트는 매 스텝마다

- 현재 상태에서 행동을 선택하고
    
- 환경으로부터 Reward를 받아
    
- 이 Reward를 이용해  
    가치 함수, 정책, Advantage 등을 업데이트한다.
    

---

### 2. 시간에 따른 Reward와 Return

Reward는 한 시점의 피드백이고,  
Return은 여러 시점에 걸친 Reward를 합친 값이다.

에이전트의 진짜 목표는  
“한 번의 Reward”가 아니라  
“시간에 따라 누적된 Reward(또는 할인된 합)”을  
최대한 크게 만드는 것이다.

- 에피소드 길이가 T일 때  
    시작 시점의 Return 예시
    
    - `G_0 = r_1 + r_2 + ... + r_T`
        
- 할인 계수 γ를 사용하는 경우
    
    - `G_t = r_{t+1} + γ r_{t+2} + γ² r_{t+3} + ...`
        

이 Return을 기반으로

- 상태 가치 V(s), 행동 가치 Q(s, a)를 정의하고
    
- 정책의 성능을 평가하는 목적 함수  
    `J(π) = E[Return]` 같은 형태를 만든다.
    

즉,

- Reward는 “순간적인 점수”
    
- Return은 “장기적인 총점”
    
- Value/Policy는 “이 총점을 최대화하는 방향으로” 학습된다.
    

---

### 3. Reward 설계 패턴

실제 문제에서 Reward를 어떻게 정의할지는  
강화학습 성공 여부에 큰 영향을 미친다.

1. Sparse vs Dense Reward
    
    - Sparse Reward
        
        - 특정 중요한 이벤트에서만 보상을 주는 방식
            
        - 예: 게임에서 이겼을 때 +1, 나머지는 0
            
        - 장점: 설계가 단순하고 목표가 명확
            
        - 단점: 학습 초기에 의미 있는 피드백을 받기 어려워 탐색이 매우 힘들 수 있다.
            
    - Dense Reward
        
        - 매 스텝마다 세밀하게 보상을 주는 방식
            
        - 예: 목표에 가까워지면 +0.1, 멀어지면 -0.1
            
        - 장점: 학습이 빠르고 안정적인 경우가 많다.
            
        - 단점: 잘못 설계하면 이상한 전략을 학습할 수 있다.
            
2. Reward shaping
    
    - 최종 목표 보상 외에  
        중간 진행 상황을 반영하는 보상을 추가해  
        학습을 도와주는 기법
        
    - 예:
        
        - 로봇이 목표에 가까워질수록 작은 양의 Reward를 주기
            
        - 서버 응답 시간이 줄어들수록 조금씩 보상 증가
            
    - 주의점:
        
        - shaping 보상이 최종 목표와 충돌하면  
            엉뚱한 행동을 강화할 수 있다.
            
3. 페널티형 Reward
    
    - “해야 하는 것”에 보상을 주기보다  
        “하면 안 되는 것”에 패널티를 주는 방식
        
    - 예:
        
        - 충돌 시 -10, 너무 큰 제어 입력 사용 시 -c × 제어량
            
    - 안전, 제약 조건, 비용 최소화 문제에서 자주 사용된다.
        
4. 정규화, 스케일 조정
    
    - Reward 값의 크기와 분포는  
        학습 안정성과 수렴 속도에 영향을 준다.
        
    - 너무 큰 값이나 매우 불균형한 분포는  
        gradient 폭주, 불안정한 업데이트를 유발할 수 있어  
        클리핑, 정규화, 스케일 조정 등을 하는 경우가 많다.
        

---

### 4. 구현 관점에서의 실무 포인트

- Reward는 “비즈니스 목표”와 직접 연결해서 설계하기
    
    - 게임: 승률, 점수, 생존 시간
        
    - 추천: 장기적인 유지율, 매출, 사용자 만족
        
    - 운영: 비용 절감, 지연 시간 감소, SLA 준수
        
- 단기적 편의가 아니라  
    장기적인 Return과 align되도록 설계하기
    
    - 예:
        
        - 광고 클릭 수만 올리는 Reward는  
            스팸성 광고를 양산할 수 있다.
            
        - 대신 장기 세션 길이, 재방문율 등을 포함한 Reward가  
            실제 목표와 더 잘 맞을 수 있다.
            
- Reward hacking 가능성을 항상 의식하기
    
    - 에이전트가 “규칙 구멍”을 찾아  
        사람이 의도하지 않은 방법으로 Reward를 올리는지  
        모니터링하고, 필요하면 Reward 정의를 수정해야 한다.
        
- [[RL 10 - Value Function (V, Q)]], [[RL 11 - Advantage Function]] 구현 시
    
    - Return 계산, bootstrap 방식(TD), n-step return 등  
        Reward를 어떻게 조합해 학습 신호로 쓸지  
        알고리즘별로 차이가 있으므로 함께 설계해야 한다.