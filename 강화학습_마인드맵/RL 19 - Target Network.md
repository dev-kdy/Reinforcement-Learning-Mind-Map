상위: [[RL 00 - Reinforcement Learning Index]]  
관련: [[RL 15 - Q-learning]], [[RL 17 - Deep Q-Network (DQN)]], [[RL 18 - Experience Replay]]

---

## What (정의)

Target Network는  
Q 네트워크와 별도로, 천천히 업데이트되는 또 하나의 Q 네트워크를 두는 기법이다.

- 온라인 Q 네트워크  
    현재 우리가 학습시키고 있는 주 네트워크.  
    상태를 입력받아 Q(s, a)를 출력하고,  
    이 네트워크의 파라미터를 계속 업데이트한다.
    
- 타깃 Q 네트워크  
    온라인 네트워크의 “복사본” 역할을 하는 네트워크.  
    파라미터를 자주 바꾸지 않고,  
    일정 주기마다 또는 조금씩만 온라인 네트워크를 따라가도록 만든다.
    

DQN([[RL 17 - Deep Q-Network (DQN)]])에서는  
TD 목표값을 만들 때 이 타깃 네트워크에서 나오는 Q값을 사용한다.

---

## Why (배경/목적)

딥러닝과 TD 부트스트랩을 그냥 결합하면  
학습이 매우 불안정해지기 쉽다.  
이유는 TD 목표값 자체가  
우리가 학습 중인 Q 네트워크에 의존하기 때문이다.

상황을 간단히 정리하면 다음과 같다.

- TD 업데이트의 목표값은 보통  
    r + γ max_a' Q(s', a')  
    같은 형태를 가진다.
    
- Q(s, a)를 근사하는 네트워크의 파라미터가  
    매 스텝마다 계속 변하면,  
    목표값에 쓰이는 Q(s', a')도 계속 요동친다.
    
- 즉, 우리가 맞추려는 타깃이  
    학습 중에 계속 움직이는 목표(moving target)가 되어  
    학습이 발산하거나, 이상한 방향으로 튈 위험이 커진다.
    

Target Network는 이 문제를 완화하기 위한 장치다.

- 목표값 계산에 사용하는 Q는  
    “한동안 고정된 네트워크”에서 나오게 하고
    
- 그동안 온라인 네트워크는  
    이 상대적으로 고정된 타깃을 향해 천천히 수렴하도록 만든다.
    
- 그리고 어느 정도 학습이 진행된 뒤에  
    타깃 네트워크를 다시 온라인 네트워크 쪽으로 갱신한다.
    

이렇게 하면

- 목표값이 너무 자주 바뀌지 않아서  
    한동안은 “고정된 타깃을 향해 학습하는 것”에 가깝게 만들 수 있고
    
- 결과적으로 DQN 학습의 안정성이 크게 향상된다.
    

Experience Replay([[RL 18 - Experience Replay]])가  
샘플을 섞어서 안정화한다면,  
Target Network는 “타깃을 느리게 움직여서” 안정화하는 축이라고 볼 수 있다.

---

## How (활용)

### 1. 두 개의 Q 네트워크 운용 구조

DQN에서 Target Network를 사용하는 구조를 개념적으로 정리하면 다음과 같다.

1. 온라인 Q 네트워크 Q_online
    
    - 입력: 현재 상태 s
        
    - 출력: 각 행동에 대한 Q_online(s, a)
        
    - 역할
        
        - 행동 선택(ε-greedy 등)에 사용
            
        - 역전파로 학습되는 주 네트워크
            
2. 타깃 Q 네트워크 Q_target
    
    - 입력: 다음 상태 s'
        
    - 출력: Q_target(s', a')
        
    - 역할
        
        - TD 목표값 계산에만 사용
            
        - 파라미터가 자주 바뀌지 않도록 관리
            

학습 루프에서

- 행동 선택: Q_online 기반으로 ε-greedy
    
- TD 타깃 계산: Q_target에서 나온 Q값으로 계산
    

이렇게 역할을 분리하는 것이 핵심이다.

---

### 2. 타깃 네트워크 업데이트 방식

타깃 네트워크는 두 가지 대표적인 방식으로 업데이트한다.

1. 하드 업데이트(hard update, periodic copy)
    
    - 일정 스텝마다  
        Q_target의 파라미터를  
        Q_online의 파라미터로 통째로 복사한다.
        
    - 개념적으로는 다음과 같은 느낌이다.
        
        - 매 N 스텝마다  
            Q_target ← Q_online
            
        - 그 사이 N 스텝 동안은  
            Q_target은 고정된 채로 사용된다.
            
    - 장점
        
        - 구현이 단순하고 직관적이다.
            
    - 단점
        
        - 업데이트 순간에 타깃이 갑자기 크게 바뀔 수 있어  
            약간의 불연속적인 변화가 생긴다.
            
2. 소프트 업데이트(Polyak averaging)
    
    - 매 스텝마다 타깃 파라미터를  
        온라인 파라미터 쪽으로 조금씩만 이동시킨다.
        
    - 개념적으로는 다음과 같은 형태를 가진다.
        
        - θ_target ← τ θ_online + (1 − τ) θ_target  
            (0 < τ ≪ 1)
            
    - 효과
        
        - 타깃 네트워크가 점진적으로, 부드럽게 온라인 네트워크를 따라가  
            목표값 변화가 더 완만해진다.
            
    - DQN 변형들, DDPG, TD3 등에서도 많이 쓰이는 방식이다.
        

두 방식 모두 “타깃을 느리게 움직인다”는 공통된 목적을 가지고 있고,  
하드/소프트는 단지 구현 선택과 하이퍼파라미터 차이일 뿐이다.

---

### 3. TD 타깃 계산에서의 사용

DQN에서 한 transition (s, a, r, s', done)에 대한 TD 타깃을 만들 때,  
Target Network는 다음과 같은 역할을 한다.

- done이 아닌 경우
    
    - 타깃 값의 개념적 형태는  
        r + γ max_a' Q_target(s', a')
        
    - 여기서 Q_target이 타깃 네트워크의 출력이다.
        
- done인 경우
    
    - 이후 보상이 없으므로  
        타깃 값을 r로만 둔다.
        

정리하면,

- 예측값 Q_online(s, a)는 온라인 네트워크에서 나오고
    
- 목표값은 Q_target(s', ·)를 기반으로 만든 뒤
    
- 손실은 (Q_online(s, a) − target)² 같은 형태로 계산된다.
    

이 구조 덕분에

- 예측을 담당하는 네트워크와
    
- 타깃을 제공하는 네트워크가 분리되어  
    “스스로 만든 타깃을 계속 쫓아다니며 발산”하는 현상을  
    어느 정도 완화할 수 있게 된다.
    

---

### 4. 설계 및 실무 관점 요약

- Target Network의 핵심 아이디어
    
    - Q를 업데이트할 때 사용하는 목표값을  
        “한동안은 고정된 네트워크”에서 가져와  
        학습을 안정화하자는 것.
        
- 구성 요소
    
    - 온라인 네트워크 Q_online:  
        행동 선택, 예측, 학습 대상
        
    - 타깃 네트워크 Q_target:  
        TD 목표값 계산 전용, 느리게 업데이트
        
- 업데이트 전략
    
    - 하드 업데이트:  
        일정 스텝마다 통째로 복사
        
    - 소프트 업데이트:  
        Polyak averaging으로 조금씩 반영
        
- Experience Replay와 함께
    
    - Replay Buffer([[RL 18 - Experience Replay]])로  
        데이터(i.i.d에 가깝게) 안정화
        
    - Target Network로  
        타깃(moving target 문제) 안정화
        
    - 두 축이 함께 DQN([[RL 17 - Deep Q-Network (DQN)]])의  
        학습을 가능하게 만드는 핵심 안정화 메커니즘이라고 정리할 수 있다.