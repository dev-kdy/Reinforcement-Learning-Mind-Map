상위: [[RL 00 - Reinforcement Learning Index]]  
관련: [[RL 01 - Reinforcement Learning]], [[RL 02 - Agent]], [[RL 04 - State]], [[RL 05 - Action & Action Space]], [[RL 06 - Reward]], [[RL 08 - Model-free vs Model-based RL]]

---

## What (정의)

Environment는  
Agent([[RL 02 - Agent]])가 상호작용하는 외부 세계로,  
Agent의 행동에 대해 다음 상태와 보상을 돌려주는 역할을 한다.

이 Environment를 수학적으로 형식화한 모델이  
MDP(Markov Decision Process, 마르코프 결정 과정)다.

MDP는 보통 다음 구성 요소로 표현한다.

- 상태 집합 S: 가능한 모든 상태들의 집합
    
- 행동 집합 A: 가능한 모든 행동들의 집합
    
- 전이 확률 P(s' | s, a) 또는 P(s', r | s, a): 상태·행동에서 다음 상태(와 보상)로 갈 확률
    
- 보상 함수 R(s, a) 또는 R(s, a, s'): 특정 상태·행동(전이)에 대한 즉시 보상
    
- 경우에 따라 할인 계수 γ와 초기 상태 분포 ρ(s₀)
    

핵심 가정은 마르코프성이다.

현재 상태 Sₜ와 현재 행동 Aₜ를 알면,  
다음 상태 Sₜ₊₁와 보상 Rₜ₊₁의 분포가  
과거 이력과 무관하게 결정된다고 가정한다.

요약하면,

- Environment는 Agent가 행동을 시도해 보는 실제/가상의 세계이고
    
- MDP는 이 Environment를  
    상태, 행동, 보상, 전이 확률 구조로 수학적으로 표현한 틀이다.
    

---

## Why (배경/목적)

강화학습 문제를 이론적으로 다루고  
어떤 정책이 최적인지 엄밀하게 정의하려면  
환경을 수학적으로 모델링할 필요가 있다.

MDP를 사용하는 이유는 다음과 같다.

1. 마르코프성에 기반한 단순한 구조
    
    - 현재 상태와 행동만 알면 되기 때문에  
        전체 과거 이력을 직접 다루지 않아도 된다.
        
    - 이 덕분에 벨만 방정식, 동적 계획법(DP) 같은  
        수학적 도구를 사용할 수 있다.
        
2. 최적 정책을 명확히 정의할 수 있음
    
    - 주어진 MDP에서 장기 누적 보상을 최대화하는 정책 π*를  
        이론적으로 정의하고 분석할 수 있다.
        
    - 어떤 조건에서 최적 정책이 결정적 정책이어도 되는지 같은 정리들이 가능하다.
        
3. Environment와 Agent의 역할 분리
    
    - Environment는 상태 전이와 보상 구조를 정의하는 세계
        
    - Agent는 그 위에서 정책을 학습하고 행동을 선택하는 존재
        
    - 이렇게 분리하면  
        같은 Environment에서 여러 Agent 알고리즘을 공정하게 비교할 수 있다.
        
4. Model-free vs Model-based RL의 기준 제공
    
    - MDP의 전이 구조 P, 보상 함수 R을  
        알고 혹은 모델링해서 쓰는지
        
    - 아니면 모르고 샘플(경험)만 보고 학습하는지
        
    - 이 차이가 바로 [[RL 08 - Model-free vs Model-based RL]]의 핵심이다.
        

결국 Environment를 MDP로 보는 것은  
강화학습 문제를 수학적으로 정식화하고,  
이론과 알고리즘을 체계적으로 전개하기 위한 출발점이다.

---

## How (활용)

### 1. MDP 관점에서 문제 정식화

현실 문제를 강화학습/MDP로 모델링할 때는  
다음 요소들을 명시적으로 정한다.

1. 상태 집합 S
    
    - 시스템의 현재 상황을 어떻게 표현할지 결정한다.
        
    - 예시
        
        - 게임: 화면 픽셀, 캐릭터 위치·체력
            
        - 로봇: 위치, 속도, 센서 값
            
        - 추천 시스템: 사용자 최근 행동, 프로필, 시간대
            
2. 행동 집합 A
    
    - Agent가 할 수 있는 행동들을 정의한다.
        
    - 예시
        
        - 게임: 왼/오/점프/공격
            
        - 로봇: 속도·각속도 명령
            
        - 추천: 어떤 아이템을 추천할지 선택
            
3. 전이 확률 P(s' | s, a)
    
    - 현재 상태·행동에서 다음 상태가 어떻게 결정되는지 나타낸다.
        
    - 실제 시스템에서는 이 확률을 수식으로 알기 어려운 경우가 많고,  
        대신 Environment 시뮬레이터나 실제 시스템이  
        샘플을 제공한다고 보는 것이 실용적이다.
        
4. 보상 함수 R(s, a, s')
    
    - 특정 상태에서 특정 행동을 했을 때  
        얼마나 좋은지, 나쁜지 숫자로 표현한다.
        
    - 예시
        
        - 점수를 얻으면 양의 보상, 충돌하면 음의 보상
            
        - 사용자가 클릭/구매하면 +1, 이탈하면 -1
            
5. 할인 계수 γ 및 에피소드 구조
    
    - 미래 보상을 현재와 비교해 어느 정도 중요하게 볼지 결정한다.
        
    - 에피소드 시작/종료 조건(초기 상태, 게임 오버, 목표 도달 등)을 정해  
        한 판의 범위를 명확히 한다.
        

S, A, P, R, γ를 정하는 순간  
그 문제는 MDP로 정식화되며,  
강화학습 알고리즘을 적용할 준비가 끝난다.

---

### 2. Gym 스타일 Environment 인터페이스

구현에서는 보통  
OpenAI Gym 스타일 인터페이스를 사용한다.

- env.reset()
    
    - Environment를 초기화하고  
        시작 상태 s₀를 반환한다.
        
- env.step(action)
    
    - Agent가 선택한 행동 aₜ를 입력으로 받아
        
        - 다음 상태 sₜ₊₁
            
        - 보상 rₜ₊₁
            
        - 에피소드 종료 여부 done
            
        - 디버깅·로그용 부가 정보 info  
            를 튜플로 반환한다.
            

강화학습 루프는 대략 다음과 같다.

```
state = env.reset()

done = False
while not done:
    action = agent.act(state)                 # 정책에 따라 행동 선택
    next_state, reward, done, info = env.step(action)
    agent.learn(state, action, reward, next_state, done)  # 경험으로 학습
    state = next_state
```

여기서 Environment는  
전이 확률 P와 보상 함수 R을  
직접 수식으로 주는 대신,  
샘플 (s, a, r, s')를 던져주는 블랙박스처럼 동작한다.

Agent는 이 샘플들만 보고  
정책과 가치 함수를 학습하게 된다.

---

### 3. Model-free vs Model-based RL과의 연결

[[RL 08 - Model-free vs Model-based RL]]에서 구분하는  
두 가지 강화학습 패러다임은  
바로 이 MDP 구조를 어떻게 다루느냐의 차이다.

1. Model-free RL
    
    - 전이 확률 P(s' | s, a)와 보상 함수 R을  
        명시적으로 추정하지 않는다.
        
    - 샘플 (s, a, r, s')만 이용해  
        가치 함수나 정책을 직접 학습한다.
        
    - 예: Q-learning, SARSA, DQN, PPO
        
2. Model-based RL
    
    - 경험으로부터 P와 R의 모델을 먼저 학습하거나,  
        이들이 이미 주어져 있다고 가정한다.
        
    - 그런 다음, 이 모델을 사용해  
        계획(planning)이나 시뮬레이션을 수행하면서  
        정책을 개선한다.
        
    - 예: Dyna-Q, World Model 계열
        

정리하면,

- MDP는 환경이 실제로 어떻게 동작하는지를  
    수학적으로 요약한 틀이고
    
- Model-free / Model-based 강화학습은  
    Agent가 이 틀의 어떤 부분까지  
    명시적으로 사용하고 학습하는지에 따라 나뉜다.