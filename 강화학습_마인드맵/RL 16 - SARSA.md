상위: [[RL 00 - Reinforcement Learning Index]]  
관련: [[RL 01 - Reinforcement Learning]], [[RL 10 - Value Function (V, Q)]], [[RL 12 - Exploration vs Exploitation]], [[RL 14 - Temporal-Difference (TD) Learning]], [[RL 15 - Q-learning]]

---

## What (정의)

SARSA는  
on-policy TD 제어 알고리즘으로,  
현재 정책이 실제로 선택한 다음 행동을 그대로 반영하여  
행동가치함수 Q(s, a)를 업데이트하는 방법이다.

이름 SARSA는 하나의 업데이트에 사용되는 다섯 요소에서 왔다.

- S: 현재 상태
    
- A: 현재 상태에서 취한 행동
    
- R: 그 행동 후에 받은 보상
    
- S': 다음 상태
    
- A': 다음 상태에서 현재 정책이 실제로 선택한 행동
    

이 다섯 개 `(S, A, R, S', A')`를 사용해  
Q(S, A)를 한 스텝씩 보정해 나가는 TD 알고리즘이 SARSA다.

요약하면, SARSA는

- 현재 정책으로 실제로 움직이면서
    
- 그 정책이 선택한 행동들을 기준으로
    
- Q(s, a)를 on-policy 방식으로 학습하는 TD 제어 알고리즘이다.
    

---

## Why (배경/목적)

SARSA의 포인트는  
“지금 사용 중인 정책(탐색 포함)을 그대로 평가하고 개선한다”는 점이다.

[[RL 15 - Q-learning]]과의 대비로 보면 동기가 더 잘 보인다.

- Q-learning
    
    - 업데이트할 때 항상 `max_a' Q(s', a')`를 사용해  
        다음 상태에서 최적 행동을 가정한다.
        
    - 실제로는 ε-greedy로 탐색 행동을 하더라도,  
        업데이트는 언제나 이상적인 greedy 정책 기준이다.
        
    - 그래서 off-policy 알고리즘이라고 부른다.
        
- SARSA
    
    - 다음 상태에서 실제로 선택된 행동 A'에 대한 Q(S', A')를 사용한다.
        
    - 즉, 탐색을 포함한 현재 정책이 실제로 걷는 궤적을  
        그대로 반영해 Q를 업데이트한다.
        
    - 그래서 on-policy 알고리즘이라고 부른다.
        

이 차이 때문에 일반적으로

- Q-learning은 더 공격적인 최적 정책을 향해 달려가는 성향
    
- SARSA는 현재 탐색 정책을 그대로 반영해서  
    다소 보수적이고 안정적인 성향
    

을 보인다.

예를 들어, 위험한 상태 근처에서  
ε-greedy 탐색 때문에 위험 행동을 할 가능성이 크다면

- Q-learning은 “이론적인 최적 행동”만 보고 업데이트하기 때문에  
    탐색이 유발하는 위험을 과소평가할 수 있다.
    
- SARSA는 탐색으로 인해 실제로 겪게 되는 위험 상태·행동까지  
    정책 평가에 포함하기 때문에  
    보다 안전한 쪽으로 정책을 학습하는 경향이 있다.
    

이런 이유로  
탐색이 포함된 실제 행동 정책 자체를  
안전하게 운용하고 싶을 때  
SARSA가 더 적합한 선택이 될 수 있다.

---

## How (활용)

### 1. 핵심 아이디어와 업데이트

SARSA는 한 스텝에서 다음과 같은 정보로 Q를 업데이트한다.

- 현재 상태: S
    
- 현재 행동: A
    
- 보상: R
    
- 다음 상태: S'
    
- 다음 행동: A' (현재 정책이 실제로 고른 행동)
    

이때 TD 업데이트의 기본 형태는

- 새로운 값 ← 현재 값 + 학습률 × (목표값 − 현재 값)
    

이며, 여기서 목표값은

- `R + γ Q(S', A')`
    

처럼 정의된다.

직관적으로 보면,

- 지금 상태 S에서 행동 A를 했을 때의 Q(S, A)를
    
- 한 스텝 뒤에 받은 보상 R과
    
- 다음 상태 S'에서 실제로 선택한 행동 A'의 가치 Q(S', A')를 이용해
    
- 조금씩 보정해 나가는 구조다.
    

이것이 [[RL 14 - Temporal-Difference (TD) Learning]]에서 말한  
“다음 상태의 추정값을 끌어와 현재 값을 보정하는 부트스트랩” 형태의  
on-policy 버전이라고 보면 된다.

---

### 2. Exploration vs Exploitation과의 연결

SARSA에서도 행동 선택은 보통  
[[RL 12 - Exploration vs Exploitation]]의 ε-greedy 전략을 사용한다.

- 현재 Q(S, ·)에서  
    확률 1−ε로 Q가 최대인 행동을 선택  
    (exploitation)
    
- 확률 ε로 랜덤 행동을 선택  
    (exploration)
    

중요한 점은

- 업데이트에 사용하는 A'가  
    이 ε-greedy 정책으로 실제로 선택된 행동이라는 것이다.
    

즉, SARSA는

- 탐색까지 포함된 현재 행동 정책을  
    그대로 가치 평가와 개선의 기준으로 삼는다.
    

그래서

- 학습 과정에서 에이전트가 실제로 경험하는 위험, 손해, 이득이  
    Q 업데이트에 모두 반영되고
    
- 최종적으로는 “탐색을 포함한 정책”의 성능을  
    더 정확하게 반영하는 방향으로 학습된다.
    

---

### 3. Q-learning과 비교 요약

정리 차원에서 Q-learning과 SARSA의 차이를 다시 정리하면 다음과 같다.

- 업데이트 대상
    
    - 둘 다 Q(S, A)를 TD 방식으로 업데이트한다.
        
- 목표값 구성
    
    - Q-learning: `R + γ max_a' Q(S', a')`
        
        - 다음 상태에서 최적 행동을 가정
            
        - off-policy
            
    - SARSA: `R + γ Q(S', A')`
        
        - 실제로 선택된 다음 행동 A'를 사용
            
        - on-policy
            
- 성향
    
    - Q-learning: 더 공격적,  
        이론적인 최적 정책을 향해 직접 수렴하는 성향
        
    - SARSA: 탐색을 포함한 실제 정책을 반영하여  
        더 보수적이고 안정적인 성향
        
- 사용 관점
    
    - 위험 상태가 뚜렷하고  
        탐색이 실제 시스템에 큰 영향을 주는 경우  
        → SARSA 접근이 더 자연스러울 수 있다.
        
    - 최적 정책 근사 자체가 더 중요하고  
        탐색은 학습 과정의 중간 현상으로 보는 경우  
        → Q-learning 계열을 더 자주 사용한다.
        

---

### 4. 설계 및 실무 관점 요약

- SARSA는 on-policy TD 제어 알고리즘으로,  
    “현재 사용하는 탐색 정책” 자체를 평가하고 개선한다.
    
- 데이터 튜플 `(S, A, R, S', A')`를  
    한 스텝씩 관측할 수 있는 환경이라면  
    구현이 간단하고 직관적이다.
    
- [[RL 15 - Q-learning]], [[RL 14 - Temporal-Difference (TD) Learning]], [[RL 12 - Exploration vs Exploitation]]을  
    SARSA와 함께 비교해 보면  
    on-policy vs off-policy, TD vs Monte Carlo,  
    exploration 전략이 정책 평가에 어떻게 반영되는지까지  
    전체 그림을 잡는 데 도움이 된다.