상위: [[RL 00 - Reinforcement Learning Index]]  
관련: [[RL 01 - Reinforcement Learning]], [[RL 02 - Agent]], [[RL 05 - Action & Action Space]], [[RL 10 - Value Function (V, Q)]], [[RL 15 - Q-learning]], [[RL 16 - SARSA]], [[RL 17 - Deep Q-Network (DQN)]], [[RL 20 - Policy Gradient (기본 PG)]]

---

## What (정의)

Exploration vs Exploitation은  
강화학습에서 에이전트가 매 순간 부딪히는 의사결정 딜레마를 말한다.

- Exploitation
    
    - 지금까지 배운 정보에 따르면  
        가장 좋아 보이는 행동을 선택하는 것
        
    - 즉, 현재 추정 상의 최선 행동을 적극 활용하는 단계
        
- Exploration
    
    - 아직 잘 모르는 행동도 일부러 시도해 보는 것
        
    - 당장은 손해일 수 있지만,  
        더 좋은 전략을 찾기 위해 정보를 수집하는 단계
        

강화학습 에이전트는  
“지금까지의 지식을 최대한 활용할 것인가,  
아니면 새로운 정보를 얻기 위해 모험을 할 것인가”를  
계속해서 균형 잡아야 한다.

---

## Why (배경/목적)

탐색이 없으면

- 에이전트는  
    초기에 운 좋게 높은 보상을 줬던 행동만 반복하게 되고
    
- 실제로는 더 좋은 행동/전략이 있어도  
    그걸 시도해 볼 기회조차 얻지 못할 수 있다.
    

예를 들어, 간단한 슬롯머신(멀티 암드 밴딧) 상황을 생각해 보면

- A 레버: 평균 보상 0.5
    
- B 레버: 평균 보상 0.6
    

초기에 몇 번 뽑았을 때 우연히

- A에서 1, 1, 1이 나와서  
    “A가 매우 좋아 보이는 상태”가 되었다면
    
- B는 아직 충분히 시도해 보지 않아서  
    실제로 더 좋은 레버임에도 저평가된 상태일 수 있다.
    

이때 탐색 없이 Exploitation만 하면

- 에이전트는 A만 계속 뽑게 되고
    
- 결국 B가 더 좋다는 사실을 영원히 못 배울 수 있다.
    

반대로 탐색만 너무 많이 하면

- 이미 충분히 안 좋은 걸로 드러난 행동을  
    계속해서 쓸데없이 시도하게 되어
    
- 성능이 계속 낮게 머무를 수 있다.
    

따라서 좋은 강화학습 에이전트는

- 학습 초반에는 Exploration 비중을 크게 두고
    
- 점점 시간이 지나면서  
    Exploitation 비중을 늘리는 식으로  
    두 가지를 균형 있게 조절해야 한다.
    

---

## How (활용)

### 1. 대표적인 탐색 전략 (이산 행동 공간)

1. ε-greedy
    
    - 확률 ε로는 랜덤 행동(탐색),  
        확률 1−ε로는 현재 Q값이 최대인 행동(활용)을 선택한다.
        
    - [[RL 15 - Q-learning]], [[RL 16 - SARSA]], [[RL 17 - Deep Q-Network (DQN)]]에서 기본적으로 쓰이는 패턴이다.
        
    - ε 스케줄
        
        - 고정 ε: 항상 같은 비율로 탐색
            
        - 감소형 ε:
            
            - 학습 초반에는 ε를 크게 두고
                
            - 시간이 지날수록 ε를 줄여  
                나중에는 Exploitation 위주로 전환
                
2. Softmax / Boltzmann 탐색
    
    - Q(s, a)를 온도 파라미터 τ로 스케일링해  
        softmax로 행동 확률을 만든 뒤, 그 확률에 따라 행동을 샘플링한다.
        
    - Q가 약간 더 큰 행동은 더 자주 선택되지만,  
        Q가 작은 행동도 완전히 배제되지는 않는다.
        
    - τ가 크면 거의 랜덤에 가깝고,  
        τ가 작으면 greedy에 가까워진다.
        
3. Optimistic initialization
    
    - 초기 Q(s, a) 값을 실제보다 높게 설정해 두면  
        처음에는 대부분의 행동이 “좋게” 보이기 때문에  
        자연스럽게 여러 행동을 탐색하게 된다.
        
    - 경험이 쌓이면 실제 보상에 맞게 값이 내려가고,  
        진짜 좋은 행동만 남게 된다.
        
4. 무작위 타이 브레이킹
    
    - 여러 행동이 같은 Q값을 가지는 경우  
        항상 첫 번째 행동을 택하는 대신  
        그들 중 하나를 랜덤하게 선택하도록 해서  
        작은 수준의 탐색을 유지한다.
        

---

### 2. 좀 더 진보된 탐색 기법 (개념 위주)

1. Upper Confidence Bound(UCB)
    
    - “기대 보상 + 불확실성 보너스” 형태의 점수를  
        각 행동에 대해 계산하고  
        이 점수가 최대인 행동을 선택한다.
        
    - 많이 시도하지 않은 행동은  
        불확실성이 크기 때문에  
        추가 탐색 대상으로 우선순위가 올라간다.
        
2. Thompson Sampling
    
    - 각 행동의 기대 보상에 대한 확률 분포를 유지하고,  
        그 분포에서 샘플링된 값이 가장 큰 행동을 선택한다.
        
    - 불확실성이 큰 행동은  
        샘플링 결과가 크게 나올 가능성이 있어  
        자연스럽게 더 자주 탐색하게 된다.
        
3. Intrinsic reward / curiosity 기반 탐색
    
    - “잘 예측되지 않는 상태”나  
        “처음 보는 상태”에 대해  
        추가적인 내부 보상을 주어  
        에이전트가 새로운 상태를 더 탐색하도록 유도한다.
        
    - 특히 고차원 환경, sparse reward 환경에서 많이 연구되는 방식이다.
        

---

### 3. 연속 행동 공간에서의 탐색

연속 Action Space에서는  
단순 ε-greedy보다는 다른 방식이 더 자연스럽다.

1. 행동에 노이즈 추가
    
    - 결정론적 정책 a = μ(s)에 대해  
        a' = μ(s) + noise  
        형태로 노이즈를 더해 탐색한다.
        
    - DDPG, TD3 등에서 많이 사용되는 패턴이다.
        
2. 확률적 정책 사용
    
    - Policy가 바로 확률 분포 π(a|s)를 출력하도록 만들고  
        그 분포에서 행동을 샘플링한다.
        
    - [[RL 20 - Policy Gradient (기본 PG)]], [[RL 23 - Proximal Policy Optimization (PPO)]] 등의 기본 방식이다.
        
    - 분포의 엔트로피를 높이는 정규화 항을 넣어  
        탐색을 유지하는 전략도 함께 사용된다.
        
3. 엔트로피 보너스
    
    - 정책 엔트로피가 높을수록(더 랜덤할수록)  
        추가 보상을 주어,  
        일정 수준의 탐색을 유지하게 만드는 방법이다.
        
    - PPO, SAC 같은 알고리즘에서 자주 등장한다.
        

---

### 4. 설계 및 실무 관점 요약

- 학습 초반 vs 후반
    
    - 초반에는 Exploration 비중을 크게 두어  
        다양한 행동을 시도하며 정보를 모으고
        
    - 후반에는 Exploitation 비중을 높여  
        지금까지 배운 최선 전략을 더 자주 사용하도록 한다.
        
- ε, 온도(τ), 노이즈 크기 등 하이퍼파라미터
    
    - 너무 작으면 탐색이 부족하고
        
    - 너무 크면 언제까지나 정책이 수렴하지 못한다.
        
    - 대개 시간에 따라 점진적으로 줄이는 스케줄을 사용한다.
        
- 환경 특성을 고려하기
    
    - 데이터 수집이 비싼 실제 시스템(로봇, 운영 시스템 등)에서는  
        무작정 랜덤 탐색을 하기 어렵다.
        
    - 이런 경우 안전 제약을 포함한 탐색 전략,  
        시뮬레이터 기반 사전 탐색,  
        오프라인 로그를 활용한 초기 정책 학습 등을  
        함께 고려해야 한다.
        
- 요약
    
    - Exploration vs Exploitation은  
        “새로운 걸 시도할 것인가,  
        아니면 지금까지 배운 최선 전략을 사용할 것인가”의 균형 문제다.
        
    - 좋은 강화학습 시스템은  
        이 균형을 시간과 환경에 맞게  
        잘 조절하는 메커니즘을 갖고 있어야 한다.