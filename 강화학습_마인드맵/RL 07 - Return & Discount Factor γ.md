상위: [[RL 00 - Reinforcement Learning Index]]  
관련: [[RL 01 - Reinforcement Learning]], [[RL 06 - Reward]], [[RL 10 - Value Function (V, Q)]], [[RL 13 - Monte Carlo Learning]], [[RL 14 - Temporal-Difference (TD) Learning]]

---

## What (정의)

Return은  
한 에피소드에서 앞으로 받게 될 보상들의 (할인된) 누적합이다.

시점 t에서의 Return `G_t`는 보통 다음처럼 정의한다.

- `G_t = r_{t+1} + r_{t+2} + r_{t+3} + ...`
    
- 또는 할인 계수 γ를 사용하면  
    `G_t = r_{t+1} + γ r_{t+2} + γ² r_{t+3} + ...`
    

여기서 γ(0 ≤ γ ≤ 1)는 Discount Factor로,  
미래 보상을 현재와 비교해 얼마나 중요하게 볼지를 결정하는 파라미터다.

- γ가 0이면 다음 스텝의 보상만 중요
    
- γ가 1에 가까울수록 아주 먼 미래의 보상도 꽤 중요하게 반영
    

요약하면,

- Reward는 “지금 한 행동에 대한 즉각적인 점수”
    
- Return은 “지금 이후로 쭉 받게 될 점수의 (할인된) 총합”
    
- γ는 “미래 점수를 얼마나 신경 쓸지 정하는 무게”다.
    

---

## Why (배경/목적)

강화학습의 목표는  
단기 보상 극대화가 아니라 장기 성과 극대화다.

하지만 다음과 같은 실질적인 문제가 있다.

1. 무한히 먼 미래까지의 보상을 그대로 합치면
    
    - 합이 수렴하지 않을 수 있다.
        
    - 수학적으로 목적 함수가 잘 정의되지 않을 수 있다.
        
2. 너무 먼 미래의 보상까지  
    현재와 똑같은 비중으로 고려하면
    
    - 학습이 불안정해질 수 있고
        
    - “언제 얻을지 모르는 막연한 보상”에 과도하게 끌릴 수 있다.
        

그래서 Discount Factor γ를 도입한다.

- γ를 곱해 줄수록  
    멀리 떨어진 시점의 보상일수록 영향력이 줄어든다.
    
- 이 덕분에  
    무한 합도 수렴시키고,  
    장기성과와 안정성 사이의 균형을 조절할 수 있다.
    

또한 γ는 사실상 “효과적인 시간 지평”을 정하는 역할을 한다.

- 대략 1 / (1 - γ) 정도의 길이까지가  
    의미 있게 고려되는 구간이라고 볼 수 있다.  
    예: γ = 0.9면 대략 10스텝 정도, γ = 0.99면 대략 100스텝 정도
    

결국 Return과 γ는

- 장기 성과를 수학적으로 표현하고
    
- 가치 함수 V, Q의 정의를 만들며
    
- [[RL 13 - Monte Carlo Learning]], [[RL 14 - Temporal-Difference (TD) Learning]] 같은  
    학습 알고리즘의 기반이 되는 핵심 개념이다.
    

---

## How (활용)

### 1. γ 값에 따른 성질

- γ = 0
    
    - 바로 다음 스텝의 Reward만 본다.
        
    - 매우 단기적인 의사결정, 일종의 “눈앞 이익만 보는” 전략이 된다.
        
- 0 < γ < 1
    
    - 일반적인 강화학습에서 가장 많이 쓰이는 구간
        
    - 가까운 미래의 보상은 크게,  
        먼 미래의 보상은 점점 작게 반영
        
- γ가 1에 아주 가까운 값
    
    - 장기적인 보상을 매우 중요하게 본다.
        
    - 하지만 학습 분산이 커지고, 수렴 속도가 느려질 수 있다.
        

실무에서는 문제 특성에 따라  
0.9, 0.95, 0.99 같은 값을 많이 사용한다.

---

### 2. Return 계산 방법과 알고리즘과의 연결

Return `G_t`는  
[[RL 10 - Value Function (V, Q)]]에서 정의하는 가치 함수와  
직접적으로 연결된다.

- 상태 가치 함수
    
    - `V^π(s) = Eπ[G_t | S_t = s]`
        
- 행동 가치 함수
    
    - `Q^π(s, a) = Eπ[G_t | S_t = s, A_t = a]`
        

이때 `G_t`를 어떻게 계산·추정하느냐에 따라  
학습 알고리즘이 갈린다.

1. Monte Carlo 방식([[RL 13 - Monte Carlo Learning]])
    
    - 한 에피소드가 끝날 때까지 기다렸다가  
        실제로 받은 Reward들을 모두 합해  
        `G_t`를 정확히 계산한다.
        
    - 장점: 편향이 없는 정확한 에피소드 Return 사용
        
    - 단점: 에피소드가 끝날 때까지 기다려야 해서  
        업데이트가 느리고, 분산이 크다.
        
2. Temporal-Difference 방식([[RL 14 - Temporal-Difference (TD) Learning]])
    
    - 실제 Reward와  
        다음 상태의 가치 추정치를 섞어서  
        Return을 근사한다.
        
    - 예: 1-step TD
        
        - `G_t ≈ r_{t+1} + γ V̂(s_{t+1})`
            
    - 장점: 한 스텝마다 바로 업데이트 가능,  
        온라인 학습에 적합
        
    - 단점: 다음 상태 가치 추정치에 의존하므로  
        편향이 존재할 수 있다.
        

이 둘을 절충한 n-step Return, λ-return 같은 개념도  
이 Return 정의를 바탕으로 나온 변형들이다.

---

### 3. γ 선택과 설계 관점 포인트

- 문제의 시간 스케일에 맞추기
    
    - 매우 짧은 상호작용(몇 스텝 안에 결과가 나는 게임 등)  
        → 상대적으로 작은 γ도 괜찮다.
        
    - 장기 전략이 중요한 문제(유저 유지, 비즈니스 수익 등)  
        → γ를 0.99처럼 크게 잡는 편이 일반적이다.
        
- 너무 큰 γ의 위험
    
    - Discount가 거의 안 되면  
        먼 미래의 불확실한 보상까지 크게 반영되어  
        학습이 불안정해질 수 있다.
        
    - 특히 함수 근사와 함께 쓰일 때  
        값이 발산하거나, 수렴 속도가 매우 느려질 수 있다.
        
- 너무 작은 γ의 위험
    
    - 당장 눈앞의 Reward만 쫓는  
        근시안적인 정책을 학습할 수 있다.
        
    - 예:
        
        - 짧은 이익을 위해 장기적인 큰 손실을 초래하는 전략 선택
            
- 실무 팁
    
    - 먼저 γ를 0.9~0.99 범위에서 시작해 보고
        
    - 학습 곡선, 정책의 행동 양상을 보면서  
        “너무 단기적인가 / 너무 장기적인가”를 판단해 조정하는 식으로 튜닝하는 경우가 많다.
        

---

### 4. Return과 가치 함수, 정책 학습의 연결

정리하면

- Reward: 매 스텝의 즉각적인 피드백
    
- Return: Reward들의 (할인된) 누적합
    
- Value Function: Return의 기댓값
    
- Policy: 이 Return(또는 Value)을 최대화하도록  
    행동 확률을 조정하는 대상
    

[[RL 15 - Q-learning]], [[RL 17 - Deep Q-Network (DQN)]] 같은  
Value-based 방법이든,  
[[RL 20 - Policy Gradient (기본 PG)]] 같은 Policy-based 방법이든,  
모두 궁극적으로는  
이 Return을 최대화하는 방향으로  
파라미터를 업데이트한다.