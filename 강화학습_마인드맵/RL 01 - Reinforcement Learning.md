상위: [[RL 00 - Reinforcement Learning Index]]  
관련: [[RL 02 - Agent]], [[RL 03 - Environment & MDP]], [[RL 06 - Reward]], [[RL 09 - Policy (정책)]], [[RL 10 - Value Function (V, Q)]]

---

## What (정의)

강화학습은  
에이전트([[RL 02 - Agent]])[^1]가 환경([[RL 03 - Environment & MDP]])[^2] 안에서 직접 행동을 해 보면서,  
그때그때 받는 보상([[RL 06 - Reward]])[^3]을 기준으로  
어떻게 행동해야 장기적으로 이득(보상의 합)을 가장 많이 얻을지 학습하는 프레임워크다.

- 매 시점 t마다 에이전트는  
  현재 상황([[RL 04 - State]]) `s_t`(상태)[^4]를 보고,  
  할 수 있는 행동들([[RL 05 - Action & Action Space]])(행동 공간)[^5] 중 하나 `a_t`를 선택한다.

- 환경은 그 행동에 대한 반응으로  
  - 다음 상태 `s_{t+1}`  
  - 그 행동이 얼마나 좋았는지 알려주는 숫자, 즉 보상 `r_{t+1}`  
    를 돌려준다.

- 에이전트는 이렇게 쌓인 경험들 `(s_t, a_t, r_{t+1}, s_{t+1})`을 이용해서  
  - 어떤 상태에서 어떤 행동을 하면 좋은지에 대한 정책([[RL 09 - Policy (정책)]])[^6]  
  - 혹은 상태/행동의 가치 함수([[RL 10 - Value Function (V, Q)]])[^7]  
    를 점점 더 똑똑하게 만든다.

- 최종 목표는 미래까지 포함한 보상의 총합([[RL 07 - Return & Discount Factor γ]])(Return, 할인 계수 γ)[^8]를 최대화하는 정책을 찾는 것이다.  
  즉, 당장 눈앞의 보상만이 아니라 장기적인 이득을 고려해서 행동하는 법을 배우는 것이 핵심이다.

---

## Why (배경/목적)

지도학습은 입력 x에 대해 정답 y가 이미 주어져 있고,  
모델은 이 정답 라벨을 맞추도록 학습한다.[^14]  
하지만 많은 의사결정 문제에서는  
각 순간의 행동마다 정답 라벨이 있는 것이 아니라,  
여러 행동이 이어진 결과에 대해서만 잘했는지 못했는지가 주어진다.

예를 들어:

- 게임에서는 한 번의 키 입력이 잘했는지 못했는지 라벨로 알려주지 않고,  
  한 판이 끝난 뒤에 승패와 최종 점수만 알 수 있다.
- 로봇 제어에서는 매 순간의 모터 제어값에 대해  
  정답 라벨을 사람이 붙이기 어렵고,  
  대신 목표에 얼마나 가까워졌는지 같은 보상만 정의하기 쉽다.
- 추천 시스템에서는 어떤 시점에 어떤 콘텐츠를 보여주는 것이  
  장기적으로 사용자 만족과 매출에 좋은지에 대한 정답 라벨을 만들기 어렵고,  
  클릭·구매·이탈 여부 같은 간접 신호만 얻을 수 있다.

이런 상황에서는  
각 시점의 행동 `a_t`에 대한 정답 라벨 대신  
환경이 돌려주는 보상 `r_t`와 장기적인 결과를 통해  
좋은 정책을 스스로 찾아가야 한다.  

강화학습은 바로 이  
행동 → 결과 → 보상 피드백 루프를 직접 이용해  
에이전트가 스스로 행동 규칙(정책)을 학습하게 하는  
학습 패러다임이다.

---

## How (활용)

### 1. 기본 루프 (Interaction Loop)

강화학습의 핵심 흐름은 다음과 같이 반복된다:

1. **관찰**  
   [[RL 02 - Agent]]가 현재 상태 `s_t`를 관찰한다.  
   (예: 게임 화면, 로봇 센서 값, 사용자의 최근 행동 로그 등)

2. **행동 선택**  
   에이전트는 현재 정책([[RL 09 - Policy (정책)]])에 따라  
   행동 공간에서 행동 `a_t`를 고른다.  

   - 예: 왼쪽으로 이동, 점프, 추천 A 노출, 로봇 팔을 일정 각도만큼 회전

3. **환경 반응**  
   [[RL 03 - Environment & MDP]]가  
   - 다음 상태 `s_{t+1}`  
   - 보상 `r_{t+1}`  
     을 돌려준다.  

   - 예: 게임에서 점수를 +10 얻음, 로봇이 목표에 더 가까워짐,  
     사용자가 추천을 클릭함(보상 +1)

4. **학습(업데이트)**  
   이 경험 `(s_t, a_t, r_{t+1}, s_{t+1})`을 모아서  
   - 가치 함수 V, Q를 업데이트하거나  
   - 정책 π를 업데이트한다.  

   구체적인 업데이트 방식이  
   [[RL 15 - Q-learning]], [[RL 17 - Deep Q-Network (DQN)]], [[RL 23 - Proximal Policy Optimization (PPO)]] 같은 알고리즘들이다.[^9]

5. **에피소드 종료 및 재시작**  
   - 게임에서 게임 오버, 로봇이 목표 지점 도달,  
     추천 시나리오 종료 등으로  
     하나의 에피소드(episode)[^10]가 끝나면  
   - 다시 초기 상태에서 새 에피소드를 시작하며  
     같은 루프를 반복한다.

---

### 2. 적용 사례

1. **게임 플레이**  
   - 상태: 현재 화면, 말의 위치, 남은 체력 등  
   - 행동: 조이스틱 방향, 점프 여부, 말 두기 위치  
   - 보상: 점수 증가, 승/패 결과, 체력 감소 등  
   - 목표: 오랫동안 플레이했을 때 평균 점수와 승률을  
     최대화하는 정책 학습

2. **로봇 제어 / 자율주행**  
   - 상태: 센서 값, 카메라 영상, 현재 속도·위치  
   - 행동: 바퀴 회전, 가속·감속, 핸들 조향  
   - 보상: 목표에 가까워지면 양의 보상,  
     장애물과 충돌하면 음의 보상  
   - 목표: 안전하고 효율적으로 목적지를 찾는 정책 학습

3. **추천 시스템 / 광고**  
   - 상태: 사용자의 최근 행동, 선호도, 시간대, 디바이스 등  
   - 행동: 어떤 콘텐츠·상품·광고를 노출할지 선택  
   - 보상: 클릭, 구매, 시청 시간, 이탈 여부 등  
   - 목표: 단발성 클릭이 아니라  
     장기적인 사용자 유지와 매출을 최대화하는 정책 학습

4. **운영 최적화 (시스템 튜닝, 재고·물류 등)**  
   - 상태: 서버 상태, 트래픽, 재고량, 주문량 등  
   - 행동: 서버 스케일 조절, 재고 발주, 배송 경로 선택  
   - 보상: 비용 절감, 지연 시간 감소, 서비스 품질 향상  
   - 목표: 시간이 지날수록 누적 비용을 최소화하거나  
     누적 이익을 최대화하는 전략 학습

---

### 3. 구현 관점에서의 활용 포인트

- **Model-free vs Model-based**[^11]  
  - 환경의 동작을 명시적으로 모델링하지 않고  
    경험만으로 학습하면 Model-free  
    (예: [[RL 15 - Q-learning]], [[RL 17 - Deep Q-Network (DQN)]])  
  - 환경의 전이 확률·보상 구조까지 모델링하면 Model-based  
    (예: 계획(planning)과 시뮬레이션 기반 방법)

- **Value-based vs Policy-based vs Actor-Critic**[^12]  
  - Value-based: Q(s, a)를 학습해 거기서 정책을 유도하는 방식  
  - Policy-based: 정책 π(a|s)를 직접 학습 (예: [[RL 22 - Policy Gradient]])  
  - Actor-Critic: 가치 함수와 정책을 함께 써서  
    서로의 단점을 보완 (예: [[RL 23 - Proximal Policy Optimization (PPO)]])  

- **온라인 학습 / 오프라인 학습**[^13]  
  - 온라인: 에이전트가 실제 환경과 상호작용하면서  
    실시간으로 학습  
  - 오프라인: 과거에 모아둔 경험 데이터셋(로그)만 가지고 학습  
    (로그 기반 RL, 추천 시스템 등)

---

[^1]: 에이전트(Agent) — 환경 안에서 행동을 선택하고 보상을 받으면서 학습하는 주체. 알고리즘이 연기하는 플레이어에 해당한다.  
[^2]: 환경(Environment) — 에이전트의 행동에 반응해서 새로운 상태와 보상을 돌려주는 세계. 게임, 로봇이 움직이는 물리 공간, 온라인 서비스 시스템 등이 이에 해당한다.  
[^3]: 보상(Reward) — 특정 행동의 즉시적인 좋고 나쁨을 숫자로 표현한 값. 값이 클수록 좋은 행동, 작거나 음수일수록 나쁜 행동으로 간주한다.  
[^4]: 상태(State) — 현재 상황을 표현하는 정보 묶음. 게임 화면, 로봇의 위치·속도, 유저의 최근 행동 로그 등이 될 수 있다.  
[^5]: 행동 공간(Action Space) — 에이전트가 선택할 수 있는 모든 행동들의 집합. 이산(왼·오·점프 등)일 수도 있고, 연속(실수값 조향각 등)일 수도 있다.  
[^6]: 정책(Policy) — 어떤 상태에서 어떤 행동을 할지 결정하는 규칙 또는 확률 분포. 보통 π(a|s) 형태로 쓴다.  
[^7]: 가치 함수(Value Function) — 특정 상태나 (상태, 행동)이 앞으로 얼마만큼의 누적 보상을 가져올지 예측하는 함수(V(s), Q(s, a)).  
[^8]: Return & 할인 계수 γ — 에피소드 동안 받는 보상의 총합(또는 할인된 총합)을 Return이라 하고, γ는 미래 보상을 얼마나 중요하게 볼지 조절하는 0~1 사이의 값이다.  
[^9]: 학습 알고리즘 — Q-learning, DQN, PPO 같은 알고리즘들이 경험을 이용해 가치 함수와 정책을 업데이트하는 구체적인 절차를 정의한다.  
[^10]: 에피소드(Episode) — 시작 상태에서 출발해 종료 조건(게임 오버, 목표 도달 등)에 이를 때까지의 한 플레이 전체를 의미한다.  
[^11]: Model-free / Model-based — 환경의 동작(전이 확률, 보상 구조)을 따로 모델링하지 않고 바로 정책·가치만 학습하면 Model-free, 환경 모델까지 학습해 시뮬레이션·계획을 쓰면 Model-based라고 부른다.  
[^12]: Value-based / Policy-based / Actor-Critic — 가치 함수만 학습해서 정책을 유도하면 Value-based, 정책을 직접 학습하면 Policy-based, 두 방식을 함께 쓰면 Actor-Critic 계열이다.  
[^13]: 온라인/오프라인 강화학습 — 실제 환경과 상호작용하면서 실시간으로 학습하면 온라인 강화학습, 과거에 쌓인 로그 데이터를 가지고만 학습하면 오프라인 강화학습이라고 부른다.  
[^14]: 지도학습(Supervised Learning) — 입력에 대한 정답 라벨이 주어진 데이터셋을 이용해, 모델이 그 라벨을 잘 맞추도록 학습하는 방식.
