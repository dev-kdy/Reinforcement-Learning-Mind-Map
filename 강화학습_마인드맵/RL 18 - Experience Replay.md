상위: [[RL 00 - Reinforcement Learning Index]]  
관련: [[RL 15 - Q-learning]], [[RL 17 - Deep Q-Network (DQN)]], [[RL 19 - Target Network]]

---

## What (정의)

Experience Replay는  
에이전트가 환경과 상호작용하며 얻은 과거 경험들을  
버퍼에 저장해 두었다가,  
학습할 때 이 버퍼에서 무작위로 샘플링해 사용하는 기법이다.

여기서 한 번의 경험(transition)은 보통 이렇게 묶어서 저장한다.

- 상태 s
    
- 행동 a
    
- 보상 r
    
- 다음 상태 s'
    
- 에피소드 종료 여부 done
    

이러한 transition들을 Replay Buffer(또는 Replay Memory)에 차곡차곡 쌓아 두고,

- 매 스텝마다 새 transition을 버퍼에 push 해 두었다가
    
- 학습 시에는 이 버퍼에서 무작위로 여러 개를 뽑아  
    미니배치 형태로 네트워크를 업데이트한다.
    

즉, “지나간 경험을 그냥 버리지 않고,  
섞어서 여러 번 재사용하는 메모리”라고 생각하면 된다.

---

## Why (배경/목적)

Experience Replay를 쓰는 이유는 크게 세 가지로 볼 수 있다.

1. 샘플 간 상관 관계 줄이기
    
    - 강화학습에서 에이전트가 얻는 경험은  
        연속된 시간축 상에서 발생하고,  
        이웃한 transition들끼리 강하게 상관되어 있다.
        
    - 이런 상관된 데이터로 바로 학습하면  
        신경망이 특정 궤적에 과도하게 맞춰지거나,  
        학습이 불안정해질 수 있다.
        
    - Replay Buffer에서 랜덤 샘플링을 하면  
        시간적으로 떨어진 경험들을 섞어서 사용하게 되어  
        i.i.d에 조금 더 가까운 학습 샘플을 만들 수 있다.
        
2. 데이터 재사용으로 샘플 효율 향상
    
    - 한 번 지나간 경험을 한 번만 쓰고 버리면  
        실제 환경과의 상호작용이 비싼 상황에서 비효율적이다.
        
    - Replay Buffer를 사용하면  
        과거 경험을 여러 번 꺼내 쓰며  
        같은 데이터로 여러 번 학습할 수 있어  
        샘플 효율을 크게 높일 수 있다.
        
3. 학습 안정성 향상
    
    - [[RL 17 - Deep Q-Network (DQN)]]처럼  
        신경망과 TD 부트스트랩을 함께 쓰는 구조에서는  
        작은 변화에도 학습이 쉽게 흔들릴 수 있다.
        
    - 다양한 시점의 경험을 섞어 학습하면  
        업데이트 방향이 한쪽으로 치우치는 것을 줄이고,  
        더 안정적으로 수렴시키는 데 도움이 된다.
        

이 세 가지 이유 때문에  
Experience Replay는 DQN 계열 알고리즘의 필수 구성 요소로 자리 잡았다.

---

## How (활용)

### 1. 기본 사용 패턴

Experience Replay의 전형적인 사용 흐름은 다음과 같다.

1. Replay Buffer 준비
    
    - 고정된 최대 크기를 가진 버퍼를 하나 만든다.
        
    - 보통 FIFO 방식으로,  
        가득 차면 가장 오래된 경험을 버리고  
        새로운 경험을 넣는다.
        
2. 경험 저장 (push)
    
    - 매 스텝 또는 매 transition마다  
        (s, a, r, s', done)을 버퍼에 추가한다.
        
3. 미니배치 샘플링
    
    - 학습할 때마다 버퍼에서  
        랜덤하게 일정 개수(batch size)의 transition을 뽑는다.
        
    - 이 미니배치를 사용해  
        신경망의 TD 손실을 계산하고  
        역전파로 파라미터를 업데이트한다.
        
4. 반복
    
    - 환경과 상호작용하면서  
        버퍼에 계속 새로운 경험을 쌓고,
        
    - 일정 스텝마다 샘플링–학습을 반복한다.
        

DQN에서는 이 구조가

- Q 네트워크 업데이트의 기본 루프가 된다.
    

---

### 2. Q-learning / DQN과의 연결

[[RL 15 - Q-learning]], [[RL 17 - Deep Q-Network (DQN)]] 관점에서 보면  
Experience Replay는 다음 역할을 한다.

- Q-learning 자체는  
    “transition 하나를 보고 Q(s, a)를 TD 업데이트한다”는 구조를 가진다.
    
- DQN에서는 이 업데이트를  
    “버퍼에서 뽑은 여러 transition에 대해  
    한 번에 미니배치로 수행”하는 형태로 바꾼다.
    

이렇게 하면

- 각 업데이트 스텝에서 다양한 상태·행동·보상 조합을 함께 보고  
    파라미터를 조정하게 되어  
    학습이 더 안정적이고,  
    신경망이 여러 부분을 동시에 조금씩 개선하게 된다.
    

요약하면

- Replay Buffer는  
    DQN이 “샘플을 섞어서, 여러 번 reuse하면서,  
    배치 학습처럼 Q를 업데이트”할 수 있도록 해 주는 핵심 장치다.
    

---

### 3. 설계 및 실무 관점 포인트

Experience Replay를 설계/튜닝할 때 자주 고려하는 요소들이다.

1. 버퍼 크기
    
    - 너무 작으면  
        최근 경험에만 과도하게 의존하고,  
        데이터 다양성이 떨어진다.
        
    - 너무 크면  
        오래된 경험이 현재 정책과 너무 동떨어질 수 있고,  
        메모리 사용량이 커진다.
        
    - 보통 실험적으로 적절한 크기를 정한다.
        
2. 학습 시작 시점
    
    - 버퍼에 경험이 거의 없는 초반부터 학습을 시작하면  
        샘플 다양성이 부족해 불안정해질 수 있다.
        
    - 일정량 이상의 데이터가 쌓인 뒤  
        학습을 시작하는 워밍업 구간을 두는 경우가 많다.
        
3. 샘플링 전략
    
    - 기본은 균일 랜덤 샘플링이다.
        
    - 나중에는 TD 오차가 큰 경험을 더 자주 뽑는  
        우선순위 기반 경험 재플레이(우선순위 Replay)를 도입하기도 한다.
        
4. on-policy vs off-policy
    
    - Experience Replay는 본질적으로  
        과거 정책으로 모은 데이터를  
        나중에 현재 정책 업데이트에 사용하는 구조라  
        off-policy 학습과 잘 맞는다.
        
    - on-policy 성질이 중요한 알고리즘에서는  
        Replay를 쓸 때 주의가 필요하거나  
        아예 사용하지 않는 경우도 있다.
        

---

정리하면, Experience Replay는

- 강화학습에서 “연속된 경험을 섞어서, 여러 번 재사용”함으로써  
    샘플 간 상관을 줄이고, 샘플 효율과 안정성을 함께 높이는 기법이고
    
- 특히 [[RL 17 - Deep Q-Network (DQN)]] 같은  
    딥러닝 기반 value-based 알고리즘에서  
    사실상 표준 구성 요소로 사용되는 메커니즘이다.